{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "PqQ7iq6Vj433",
    "outputId": "e4e2a54b-85a1-441a-9242-b321cb7ba851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMYVn0oFnK6E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rga5OMrojua8"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import torch\n",
    "import torch.optim \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, *meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "    def print(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def get_optimizer(model, state_dict=None):\n",
    "    optimizer = None\n",
    "    if optim == 'sgd':\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-5,\n",
    "            nesterov=True\n",
    "        )\n",
    "    elif optim == 'adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr\n",
    "        )\n",
    "    elif optim == 'rmsprop':\n",
    "        pass\n",
    "    if state_dict:\n",
    "        optimizer.load_state_dict(state_dict)\n",
    "    return optimizer\n",
    "\n",
    "# from imagenet example of pytorch: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch,  steps=(20, 40), dec_rate=0.1):\n",
    "    \"\"\"\n",
    "    Decreases the learning rate to the initial LR decayed by dec_rate every given step\n",
    "    If steps is an integer, decreases lr every <steps> epoch by dec_rate\n",
    "    \"\"\"\n",
    "    assert type(steps) in [list, tuple, int]\n",
    "    changed_flag = False\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    if type(steps) == int:\n",
    "        if epoch % steps == 0:\n",
    "            changed_flag = True\n",
    "    else:\n",
    "        if epoch in steps:\n",
    "            changed_flag = True\n",
    "    if changed_flag:\n",
    "        lr *= dec_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('Decreasing learning rate to {:1.5f}'.format(lr))\n",
    "            param_group['lr'] = lr\n",
    "    # step = 20\n",
    "    # if lr * (0.1 ** (epoch // step)) > 1e-5:\n",
    "    #     lr = lr * (0.1 ** (epoch // step))\n",
    "    #     for param_group in optimizer.param_groups:\n",
    "    #         if param_group['lr'] != lr:\n",
    "    #             print('Decreasing learning rate to {:1.5f}'.format(lr))\n",
    "    #         param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(states, output_dir, is_best=False, filename='checkpoint.pth'):\n",
    "    if is_best:\n",
    "        torch.save(states, filename + '_model_best.pth')\n",
    "    else:\n",
    "        torch.save(states, filename + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1B5c0diSjubA"
   },
   "outputs": [],
   "source": [
    "# Initialize paths\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "this_dir = osp.dirname('__file__')\n",
    "lib_path = osp.join(this_dir, '..')\n",
    "add_path(lib_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsULBmlljubD"
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from utils import _init_paths\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train_epoch(epoch, train_loader, model, criterion, optimizer, use_cuda=True):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader), batch_time, data_time,\n",
    "        top1, top5, losses, prefix=\"Epoch: [{}]\".format(epoch + 1))\n",
    "    print_freq = len(train_loader) // 4 + 1\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for i, (paths, inputs, labels) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        data_time.update(time.time() - end)\n",
    "        # forward + backward + optimize\n",
    "        if type(model).__name__ == 'Inception3' and model.aux_logits:\n",
    "            outputs, aux_outputs = model(inputs)\n",
    "            loss_aux = criterion(aux_outputs, labels)\n",
    "            loss_final = criterion(outputs, labels)\n",
    "            loss = loss_final + 0.4*loss_aux\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "        # for confusion matrix calculation\n",
    "        _, preds = outputs.topk(1, 1, True, True)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        # print statistics\n",
    "        if i % print_freq == 0 or i + 1 == len(train_loader):\n",
    "            progress.print(i+1)\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "    return top1.avg, top5.avg\n",
    "\n",
    "def validate_epoch(val_loader, model, criterion, use_cuda=True):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = utils.ProgressMeter(len(val_loader), batch_time, top1, top5, losses,\n",
    "                                   prefix='Val: ')\n",
    "    # switch to evaluate mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    print_freq = len(val_loader) // 4 + 1\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (_, inputs, labels) in enumerate(val_loader):\n",
    "            if use_cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(acc1[0], inputs.size(0))\n",
    "            top5.update(acc5[0], inputs.size(0))\n",
    "            # for confusion matrix calculation\n",
    "            _, preds = outputs.topk(1, 1, True, True)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if i % print_freq == 0 or i+1 == len(val_loader):\n",
    "                progress.print(i+1)\n",
    "        print(confusion_matrix(all_labels, all_preds))\n",
    "        return top1.avg, top5.avg\n",
    "\n",
    "def test_cassava(test_loader, model, class_names):\n",
    "    end = time.time()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    image_names = []\n",
    "    softmax_outs = []\n",
    "    report_freq = len(test_loader) // 9 + 1\n",
    "    with torch.no_grad():\n",
    "        # test is done with batch_size=1\n",
    "        for i, (path, inputs, label) in enumerate(test_loader):\n",
    "            # print(path)\n",
    "            image_names.append(path[0].split('/')[-1])\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            if tencrop_test:\n",
    "                bs, ncrops, c, h, w = inputs.size()\n",
    "                outputs = model(inputs.view(-1, c, h, w))  # fuse batch size and ncrop\n",
    "                outputs = outputs.view(bs, ncrops, -1).mean(1)  # avg over crops\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "            softmax_outs.extend(F.softmax(outputs).cpu().numpy())\n",
    "            _, pred = outputs.topk(1, 1, True, True)\n",
    "            preds.append(class_names[pred])\n",
    "\n",
    "            if i % report_freq == 0 or i+1 == len(test_loader):\n",
    "                print('Processed test data: ', i+1)\n",
    "    results_dict = {'Category': preds, 'Id': image_names}\n",
    "    results_df = pd.DataFrame(results_dict)\n",
    "    submission_file = 'Epoch{}_{}_{}_{}_{}{}{}'.format(\n",
    "        num_epochs, model_input_size, arch,\n",
    "        optim, batch_size, '_subset' if subset_finetune else '',\n",
    "        '_weightedloss' if use_weighted_loss else '')\n",
    "    i = 1\n",
    "    # if there is another file with same name, find a new name using i variable in filename\n",
    "    while os.path.exists(str(i) + '_' + submission_file + '.csv'):\n",
    "        i += 1\n",
    "    # export prediction in requested kaggle competition format\n",
    "    results_df.to_csv(str(i) + '_' + submission_file + '.csv', index=False)\n",
    "    # save softmax outputs for later ensembling\n",
    "    np.save(str(i) + submission_file, np.asarray(softmax_outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2t4MqdhjubF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "def has_file_allowed_extension(filename, extensions):\n",
    "    \"\"\"Checks if a file is an allowed extension.\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "        extensions (iterable of strings): extensions to consider (lowercase)\n",
    "    Returns:\n",
    "        bool: True if the filename ends with one of given extensions\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(ext) for ext in extensions)\n",
    "\n",
    "# Adapted from pytorch folder.py that contains DatasetFolder and ImageFolder\n",
    "def make_dataset(root, paths_dict, class_to_idx, extensions):\n",
    "    \"\"\"\n",
    "    Generates (path, label) tuples for each of the sample and returns as a list\n",
    "    :param root: root folder of given dataset split\n",
    "    :param paths_dict: contains class names as keys and image paths as values\n",
    "    :param class_to_idx: class to index dictionary\n",
    "    :param extensions: extensions to distinguish images\n",
    "    :return: images as tuple in form (path, class id)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    root = os.path.join(root, 'train')\n",
    "    root = os.path.expanduser(root)\n",
    "    for target in sorted(class_to_idx.keys()):\n",
    "        class_dir = os.path.join(root, target)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for fname in sorted(paths_dict[target]):\n",
    "            if has_file_allowed_extension(fname, extensions):\n",
    "                path = os.path.join(class_dir, fname)\n",
    "                item = (path, class_to_idx[target])\n",
    "                images.append(item)\n",
    "    return images\n",
    "\n",
    "def _create_paths_dict(root_dir, split, split_percentage, seed=None):\n",
    "    \"\"\"\n",
    "    :param root_dir:\n",
    "    :param split: split type 'train' or 'val\n",
    "    :param split_percentage:\n",
    "    :return paths_dict: a dictionary consisting of class names as\n",
    "                        keys and image paths as a list for each key\n",
    "    \"\"\"\n",
    "    split_dir = os.path.join(root_dir, 'train')  # get both train and validation data from train dir\n",
    "    class_names = sorted(os.listdir(split_dir))\n",
    "    class_names = [ch for ch in class_names if '.' not in ch]  # remove folder name if contains dot(.) in\n",
    "    paths_dict = dict()\n",
    "    total = 0\n",
    "    for class_name in class_names:\n",
    "        # for consistent random split generation between runs, set seed\n",
    "        np.random.seed(seed)\n",
    "        class_dir = os.path.join(split_dir, class_name)\n",
    "        image_names = sorted(os.listdir(class_dir))\n",
    "        num_images = len(image_names)\n",
    "        total += num_images\n",
    "        all_indices = np.array(np.random.permutation(range(num_images)))\n",
    "        if split == 'val':  # given percentage is for val, so split for train and get the rest\n",
    "            split_percentage = 1 - split_percentage  # get train set split percentage\n",
    "        split_border = np.int(num_images * split_percentage)\n",
    "        split_ind = all_indices[:split_border] if split == 'train' else all_indices[split_border:]\n",
    "        split_paths = [os.path.join(class_dir, image_names[i]) for i in split_ind]\n",
    "        paths_dict[class_name] = split_paths\n",
    "    return paths_dict\n",
    "\n",
    "def _compute_class_weights(paths_dict, class_to_idx):\n",
    "    y_train = []\n",
    "    for class_name in sorted(paths_dict.keys()):\n",
    "        y_train.extend([class_to_idx[class_name]] * len(paths_dict[class_name]))\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_train),  y_train)\n",
    "    return class_weights\n",
    "\n",
    "class CassavaTestFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(CassavaTestFolder, self).__init__(root=root, transform=transform)\n",
    "    def __getitem__(self, item):\n",
    "        path, target = self.imgs[item]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return path, img, target\n",
    "\n",
    "class CassavaFolder(ImageFolder):\n",
    "    def __init__(self, root, split, split_percentage=0.8, transform=None):\n",
    "        \"\"\"\n",
    "        A variation of ImageFolder class of pytorch adapted to cassava dataset\n",
    "        :param root: root directory of cassava dataset that contains train-test-extraimages splits\n",
    "        :param split: 'train', 'val' or 'extraimages'\n",
    "        :param split_percentage: 'used only for 'train' and 'val' sets if needed\n",
    "        :param transform: a series of pytorch transforms to apply to images\n",
    "        \"\"\"\n",
    "        super(CassavaFolder, self).__init__(root=root, transform=transform)\n",
    "        if split in ['train', 'val']:\n",
    "            # given a seed for generating the same train and val splits in different runs\n",
    "            paths_dict = _create_paths_dict(root, split, split_percentage, seed=12)\n",
    "        else:  # extraimages\n",
    "            # extraimages are classified by a model that scores 91.456 on public leaderboard\n",
    "            # only samples which are classified above a given confidence threshold are considered\n",
    "            extraimages_threshold = 0.99\n",
    "            extraimages_json = 'extraimages_above_threshold_{}.json'.format(extraimages_threshold)\n",
    "            extraimages_json = os.path.join(root, extraimages_json)\n",
    "            with open(extraimages_json, 'r') as fp:\n",
    "                paths_dict = json.load(fp)\n",
    "        self.classes = sorted(paths_dict.keys())\n",
    "        self.class_to_idx = dict(zip(self.classes, range(len(self.classes))))\n",
    "        self.samples = self.imgs = make_dataset(root, paths_dict, self.class_to_idx, self.extensions)\n",
    "        self.class_weights = _compute_class_weights(paths_dict, self.class_to_idx)\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Returns path of the image as extra to ImageFolder class\n",
    "        :param item:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[item]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return path, img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7Nygi5FjubI"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "# from dataloaders import cassava_folder\n",
    "\n",
    "# pre-calculated from training data\n",
    "mean_v = [0.4478, 0.4967, 0.3218]\n",
    "std_v = [0.2053, 0.2062, 0.1792]\n",
    "\n",
    "competition_root_path = '/content/drive/My Drive/Kaggle/data'\n",
    "\n",
    "\n",
    "def get_dataloader(data_split, train_percentage=0.8):\n",
    "    # initialize datasets and dataloaders\n",
    "    # resize_res: 256 for 224, 512 for 448, 640 for 560 (to crop from a higher resolution)\n",
    "    resize_res = int(model_input_size * 1000 / 875)\n",
    "    print('Transform resize resolution: ', resize_res)\n",
    "    mean_vec = mean_v\n",
    "    std_vec = std_v\n",
    "    dataset = loader = None\n",
    "    dir_path = os.path.dirname('__file__')\n",
    "    if data_split == 'train':\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomResizedCrop(model_input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # transforms.RandomVerticalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_vec, std=std_vec)\n",
    "        ])\n",
    "        dataset = CassavaFolder(\n",
    "            root=dir_path + competition_root_path, split='train',\n",
    "            split_percentage=train_percentage, transform=train_transform)\n",
    "        num_train_samples = len(dataset)\n",
    "        if use_extraimages:\n",
    "            extra_dataset = CassavaFolder(\n",
    "                root=dir_path + competition_root_path, split='extraimages',\n",
    "                transform=train_transform)\n",
    "            print(\"Number of extra samples: \", len(extra_dataset))\n",
    "            dataset = torch.utils.data.ConcatDataset[dataset, extra_dataset]\n",
    "            dataset.classes = dataset[0].classes\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size,\n",
    "            shuffle=True, num_workers=4, pin_memory=True)\n",
    "        print(\"Number of training samples: \", num_train_samples)\n",
    "        if use_extraimages:\n",
    "            print(\"Number of combined training samples: \", len(dataset))\n",
    "        print(\"Number of classes: \", len(dataset.classes))\n",
    "    elif data_split == 'val':\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize(resize_res),\n",
    "            transforms.CenterCrop(model_input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_vec, std=std_vec)\n",
    "        ])\n",
    "        dataset = CassavaFolder(\n",
    "            root=dir_path + competition_root_path, split='val',\n",
    "            split_percentage=1-train_percentage, transform=val_transform)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size,\n",
    "            shuffle=False, num_workers=4, pin_memory=True)\n",
    "        print(\"Number of validation samples: \", len(dataset))\n",
    "        print(\"Number of classes: \", len(dataset.classes))\n",
    "    elif data_split == 'test':\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize(resize_res),\n",
    "            transforms.CenterCrop(model_input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_vec, std=std_vec)\n",
    "        ])\n",
    "        dataset = CassavaTestFolder(\n",
    "            root=dir_path + competition_root_path + '/test', transform=test_transform)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=1, shuffle=False,\n",
    "            num_workers=4, pin_memory=True)\n",
    "        print(\"Number of test samples: \", len(dataset))\n",
    "\n",
    "    return dataset, loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5MXANvTQSMf"
   },
   "outputs": [],
   "source": [
    "#####resnet#####\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def load_pretrained_network(self, arch, dataset_numclasses):\n",
    "        print('Loading pretrained model from {}..', model_urls[arch])\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=True)\n",
    "        self.load_state_dict(state_dict)\n",
    "        if self.num_classes != dataset_numclasses:\n",
    "            self.num_classes = dataset_numclasses\n",
    "            self.fc = nn.Linear(self.fc.in_features, dataset_numclasses)\n",
    "\n",
    "    def load_finetuned_network(self, dataset_numclasses, state_dict):\n",
    "        if self.num_classes != dataset_numclasses:\n",
    "            self.num_classes = dataset_numclasses\n",
    "            self.fc = nn.Linear(self.fc.in_features, dataset_numclasses)\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    'block': {\n",
    "        'resnet18': BasicBlock, 'resnet34': BasicBlock, 'resnet50': Bottleneck,\n",
    "        'resnet101': Bottleneck, 'resnet152': Bottleneck,\n",
    "    },\n",
    "    'planes': {\n",
    "        'resnet18': [2, 2, 2, 2], 'resnet34': [3, 4, 6, 3], 'resnet50': [3, 4, 6, 3],\n",
    "        'resnet101': [3, 4, 23, 3], 'resnet152': [3, 8, 36, 3],\n",
    "    },\n",
    "}\n",
    "\n",
    "def resnet(arch, dataset_numclasses, state_dict=None, pretrained=True, **kwargs):\n",
    "    model = ResNet(model_params['block'][arch], model_params['planes'][arch], **kwargs)\n",
    "    if state_dict:\n",
    "        model.load_finetuned_network(dataset_numclasses, state_dict)\n",
    "    elif pretrained:\n",
    "        model.load_pretrained_network(arch, dataset_numclasses)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_lZOZpejubT"
   },
   "outputs": [],
   "source": [
    "def generate_model(arch, dataset_numclasses, state_dict=None, use_cuda=True):\n",
    "    if arch == 'inceptionv4':\n",
    "        model = inceptionv4(dataset_numclasses, state_dict)\n",
    "    elif arch == 'resnet34':\n",
    "        model = resnet(arch, dataset_numclasses, state_dict)\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d0f94e8c24fc4bd98500ee0db8a75206",
      "9b3607084006468fa31902b0c287adaa",
      "04629bbb6ed94433bc54dbcdb761a413",
      "067a6b4d51e244d4b1db17212bbe22f0",
      "b3f88044b56944c092c66dd0a91f6f94",
      "78c2ee8dacd0426881974c758377b826",
      "e8d312a677ce4d2ab5eea62dff58285e",
      "6b449d26691a427d9090732671c32f4f"
     ]
    },
    "colab_type": "code",
    "id": "wqJ81rwFjubV",
    "outputId": "be84da0e-395d-4fbe-bf21-713f6dcb448b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and dataloader..\n",
      "Transform resize resolution:  640\n",
      "Number of training samples:  5656\n",
      "Number of classes:  5\n",
      "Transform resize resolution:  640\n",
      "Number of test samples:  3774\n",
      "Loading pretrained model from {}.. https://download.pytorch.org/models/resnet34-333f7ec4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f94e8c24fc4bd98500ee0db8a75206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87306240), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate: {:1.5f} 0.01\n",
      "Epoch: 1 / 100\n",
      "Epoch: [1][  1/707]\tTime  6.133 ( 6.133)\tData  3.819 ( 3.819)\tAcc@1  12.50 ( 12.50)\tAcc@5 100.00 (100.00)\tLoss 1.8764e+00 (1.8764e+00)\n",
      "Epoch: [1][178/707]\tTime  0.333 ( 0.871)\tData  0.206 ( 0.732)\tAcc@1  37.50 ( 49.72)\tAcc@5 100.00 (100.00)\tLoss 2.0387e+00 (1.4524e+00)\n",
      "Epoch: [1][355/707]\tTime  0.339 ( 0.843)\tData  0.212 ( 0.710)\tAcc@1  62.50 ( 54.44)\tAcc@5 100.00 (100.00)\tLoss 1.0291e+00 (1.3090e+00)\n",
      "Epoch: [1][532/707]\tTime  0.321 ( 0.840)\tData  0.197 ( 0.709)\tAcc@1  62.50 ( 55.78)\tAcc@5 100.00 (100.00)\tLoss 8.6096e-01 (1.2406e+00)\n",
      "Epoch: [1][707/707]\tTime  0.333 ( 0.837)\tData  0.210 ( 0.707)\tAcc@1  50.00 ( 57.46)\tAcc@5 100.00 (100.00)\tLoss 1.2383e+00 (1.1796e+00)\n",
      "[[  25  275   22  136    8]\n",
      " [  50  895   61  422   15]\n",
      " [  10  169  146  441    7]\n",
      " [  17  333  137 2162    9]\n",
      " [   6  150   22  116   22]]\n",
      "Epoch 1 perf acc@1: 57.46110534667969, perf acc@5: 100.0\n",
      "Best perf acc@1: 57.46110534667969, perf acc@5: 100.0 at epoch 1\n",
      "Epoch: 2 / 100\n",
      "Epoch: [2][  1/707]\tTime  1.270 ( 1.270)\tData  1.112 ( 1.112)\tAcc@1  37.50 ( 37.50)\tAcc@5 100.00 (100.00)\tLoss 2.3404e+00 (2.3404e+00)\n",
      "Epoch: [2][178/707]\tTime  0.349 ( 0.346)\tData  0.206 ( 0.212)\tAcc@1  87.50 ( 64.19)\tAcc@5 100.00 (100.00)\tLoss 3.8204e-01 (9.8404e-01)\n",
      "Epoch: [2][355/707]\tTime  0.350 ( 0.343)\tData  0.207 ( 0.209)\tAcc@1  62.50 ( 65.70)\tAcc@5 100.00 (100.00)\tLoss 8.1777e-01 (9.5348e-01)\n",
      "Epoch: [2][532/707]\tTime  0.344 ( 0.342)\tData  0.213 ( 0.209)\tAcc@1  50.00 ( 66.68)\tAcc@5 100.00 (100.00)\tLoss 9.9200e-01 (9.2799e-01)\n",
      "Epoch: [2][707/707]\tTime  0.339 ( 0.341)\tData  0.214 ( 0.208)\tAcc@1  62.50 ( 67.86)\tAcc@5 100.00 (100.00)\tLoss 1.4359e+00 (9.0794e-01)\n",
      "[[  70  230   32  109   25]\n",
      " [  53  998   51  290   51]\n",
      " [  10   86  375  291   11]\n",
      " [  16  238   99 2279   26]\n",
      " [   7  100    8   85  116]]\n",
      "Epoch 2 perf acc@1: 67.85714721679688, perf acc@5: 100.0\n",
      "Best perf acc@1: 67.85714721679688, perf acc@5: 100.0 at epoch 2\n",
      "Decreasing learning rate to 0.00900\n",
      "Epoch: 3 / 100\n",
      "Epoch: [3][  1/707]\tTime  1.243 ( 1.243)\tData  1.093 ( 1.093)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 4.3875e-01 (4.3875e-01)\n",
      "Epoch: [3][178/707]\tTime  0.345 ( 0.346)\tData  0.211 ( 0.212)\tAcc@1  62.50 ( 72.82)\tAcc@5 100.00 (100.00)\tLoss 1.3699e+00 (7.6995e-01)\n",
      "Epoch: [3][355/707]\tTime  0.344 ( 0.343)\tData  0.204 ( 0.209)\tAcc@1  75.00 ( 71.97)\tAcc@5 100.00 (100.00)\tLoss 5.5637e-01 (7.7743e-01)\n",
      "Epoch: [3][532/707]\tTime  0.341 ( 0.342)\tData  0.211 ( 0.207)\tAcc@1  62.50 ( 72.56)\tAcc@5 100.00 (100.00)\tLoss 1.0218e+00 (7.7022e-01)\n",
      "Epoch: [3][707/707]\tTime  0.336 ( 0.342)\tData  0.212 ( 0.207)\tAcc@1 100.00 ( 72.40)\tAcc@5 100.00 (100.00)\tLoss 2.2937e-01 (7.7301e-01)\n",
      "[[  94  224   35   89   24]\n",
      " [  53 1084   42  218   46]\n",
      " [   7   64  438  248   16]\n",
      " [   8  163  116 2333   38]\n",
      " [   6   77   12   75  146]]\n",
      "Epoch 3 perf acc@1: 72.40099334716797, perf acc@5: 100.0\n",
      "Best perf acc@1: 72.40099334716797, perf acc@5: 100.0 at epoch 3\n",
      "Epoch: 4 / 100\n",
      "Epoch: [4][  1/707]\tTime  1.371 ( 1.371)\tData  1.229 ( 1.229)\tAcc@1  50.00 ( 50.00)\tAcc@5 100.00 (100.00)\tLoss 1.2857e+00 (1.2857e+00)\n",
      "Epoch: [4][178/707]\tTime  0.334 ( 0.345)\tData  0.206 ( 0.212)\tAcc@1  75.00 ( 71.00)\tAcc@5 100.00 (100.00)\tLoss 7.7348e-01 (8.2191e-01)\n",
      "Epoch: [4][355/707]\tTime  0.337 ( 0.342)\tData  0.208 ( 0.209)\tAcc@1  75.00 ( 73.27)\tAcc@5 100.00 (100.00)\tLoss 7.4497e-01 (7.6015e-01)\n",
      "Epoch: [4][532/707]\tTime  0.337 ( 0.341)\tData  0.204 ( 0.209)\tAcc@1  87.50 ( 72.86)\tAcc@5 100.00 (100.00)\tLoss 6.7434e-01 (7.6920e-01)\n",
      "Epoch: [4][707/707]\tTime  0.334 ( 0.340)\tData  0.213 ( 0.208)\tAcc@1  62.50 ( 73.53)\tAcc@5 100.00 (100.00)\tLoss 7.4725e-01 (7.4846e-01)\n",
      "[[ 122  199   39   82   24]\n",
      " [  77 1071   45  201   49]\n",
      " [   9   63  445  236   20]\n",
      " [   7  160   91 2369   31]\n",
      " [  12   73   11   68  152]]\n",
      "Epoch 4 perf acc@1: 73.53253173828125, perf acc@5: 100.0\n",
      "Best perf acc@1: 73.53253173828125, perf acc@5: 100.0 at epoch 4\n",
      "Decreasing learning rate to 0.00810\n",
      "Epoch: 5 / 100\n",
      "Epoch: [5][  1/707]\tTime  1.166 ( 1.166)\tData  0.988 ( 0.988)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 8.9047e-01 (8.9047e-01)\n",
      "Epoch: [5][178/707]\tTime  0.340 ( 0.344)\tData  0.205 ( 0.212)\tAcc@1  25.00 ( 76.05)\tAcc@5 100.00 (100.00)\tLoss 1.9272e+00 (6.6695e-01)\n",
      "Epoch: [5][355/707]\tTime  0.340 ( 0.342)\tData  0.206 ( 0.210)\tAcc@1  75.00 ( 75.88)\tAcc@5 100.00 (100.00)\tLoss 6.9522e-01 (6.7731e-01)\n",
      "Epoch: [5][532/707]\tTime  0.340 ( 0.341)\tData  0.211 ( 0.209)\tAcc@1  50.00 ( 76.67)\tAcc@5 100.00 (100.00)\tLoss 1.4910e+00 (6.6606e-01)\n",
      "Epoch: [5][707/707]\tTime  0.334 ( 0.341)\tData  0.211 ( 0.209)\tAcc@1  87.50 ( 76.57)\tAcc@5 100.00 (100.00)\tLoss 6.6147e-01 (6.6619e-01)\n",
      "[[ 168  175   27   67   29]\n",
      " [  74 1122   42  176   29]\n",
      " [  10   48  504  200   11]\n",
      " [  10  139   99 2368   42]\n",
      " [   9   65   16   57  169]]\n",
      "Epoch 5 perf acc@1: 76.57355499267578, perf acc@5: 100.0\n",
      "Best perf acc@1: 76.57355499267578, perf acc@5: 100.0 at epoch 5\n",
      "Epoch: 6 / 100\n",
      "Epoch: [6][  1/707]\tTime  1.241 ( 1.241)\tData  1.095 ( 1.095)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 2.0834e-01 (2.0834e-01)\n",
      "Epoch: [6][178/707]\tTime  0.340 ( 0.345)\tData  0.208 ( 0.212)\tAcc@1  75.00 ( 79.14)\tAcc@5 100.00 (100.00)\tLoss 6.5157e-01 (6.3543e-01)\n",
      "Epoch: [6][355/707]\tTime  0.340 ( 0.342)\tData  0.210 ( 0.210)\tAcc@1  50.00 ( 78.73)\tAcc@5 100.00 (100.00)\tLoss 1.0756e+00 (6.2567e-01)\n",
      "Epoch: [6][532/707]\tTime  0.335 ( 0.341)\tData  0.206 ( 0.209)\tAcc@1  62.50 ( 78.36)\tAcc@5 100.00 (100.00)\tLoss 1.4655e+00 (6.3227e-01)\n",
      "Epoch: [6][707/707]\tTime  0.338 ( 0.340)\tData  0.216 ( 0.209)\tAcc@1  75.00 ( 78.34)\tAcc@5 100.00 (100.00)\tLoss 5.4708e-01 (6.2719e-01)\n",
      "[[ 177  171   32   66   20]\n",
      " [  77 1126   45  158   37]\n",
      " [  15   43  528  168   19]\n",
      " [   8  126   83 2407   34]\n",
      " [   5   53   14   51  193]]\n",
      "Epoch 6 perf acc@1: 78.34158325195312, perf acc@5: 100.0\n",
      "Best perf acc@1: 78.34158325195312, perf acc@5: 100.0 at epoch 6\n",
      "Decreasing learning rate to 0.00729\n",
      "Epoch: 7 / 100\n",
      "Epoch: [7][  1/707]\tTime  1.139 ( 1.139)\tData  0.967 ( 0.967)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 6.8494e-01 (6.8494e-01)\n",
      "Epoch: [7][178/707]\tTime  0.334 ( 0.343)\tData  0.207 ( 0.211)\tAcc@1  87.50 ( 80.62)\tAcc@5 100.00 (100.00)\tLoss 4.8484e-01 (5.8205e-01)\n",
      "Epoch: [7][355/707]\tTime  0.351 ( 0.341)\tData  0.203 ( 0.210)\tAcc@1 100.00 ( 80.77)\tAcc@5 100.00 (100.00)\tLoss 1.6232e-01 (5.6885e-01)\n",
      "Epoch: [7][532/707]\tTime  0.329 ( 0.340)\tData  0.194 ( 0.209)\tAcc@1  62.50 ( 79.58)\tAcc@5 100.00 (100.00)\tLoss 9.1126e-01 (5.9661e-01)\n",
      "Epoch: [7][707/707]\tTime  0.335 ( 0.340)\tData  0.213 ( 0.209)\tAcc@1  87.50 ( 79.56)\tAcc@5 100.00 (100.00)\tLoss 7.7199e-01 (5.8730e-01)\n",
      "[[ 188  155   31   68   24]\n",
      " [  79 1140   36  143   45]\n",
      " [   4   42  544  174    9]\n",
      " [   9   99   81 2436   33]\n",
      " [   7   60    6   51  192]]\n",
      "Epoch 7 perf acc@1: 79.56153106689453, perf acc@5: 100.0\n",
      "Best perf acc@1: 79.56153106689453, perf acc@5: 100.0 at epoch 7\n",
      "Epoch: 8 / 100\n",
      "Epoch: [8][  1/707]\tTime  1.156 ( 1.156)\tData  1.018 ( 1.018)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 7.2020e-01 (7.2020e-01)\n",
      "Epoch: [8][178/707]\tTime  0.340 ( 0.344)\tData  0.211 ( 0.212)\tAcc@1  75.00 ( 80.55)\tAcc@5 100.00 (100.00)\tLoss 5.5014e-01 (5.8211e-01)\n",
      "Epoch: [8][355/707]\tTime  0.343 ( 0.342)\tData  0.208 ( 0.210)\tAcc@1  87.50 ( 80.14)\tAcc@5 100.00 (100.00)\tLoss 1.1218e+00 (5.8320e-01)\n",
      "Epoch: [8][532/707]\tTime  0.337 ( 0.341)\tData  0.208 ( 0.209)\tAcc@1  87.50 ( 80.85)\tAcc@5 100.00 (100.00)\tLoss 2.5327e-01 (5.6433e-01)\n",
      "Epoch: [8][707/707]\tTime  0.339 ( 0.341)\tData  0.216 ( 0.209)\tAcc@1  87.50 ( 80.69)\tAcc@5 100.00 (100.00)\tLoss 2.6437e-01 (5.7072e-01)\n",
      "[[ 195  144   38   65   24]\n",
      " [  66 1164   42  147   24]\n",
      " [  10   39  555  159   10]\n",
      " [   8  107   67 2439   37]\n",
      " [  11   44    9   41  211]]\n",
      "Epoch 8 perf acc@1: 80.69306945800781, perf acc@5: 100.0\n",
      "Best perf acc@1: 80.69306945800781, perf acc@5: 100.0 at epoch 8\n",
      "Decreasing learning rate to 0.00656\n",
      "Epoch: 9 / 100\n",
      "Epoch: [9][  1/707]\tTime  1.182 ( 1.182)\tData  1.025 ( 1.025)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.2119e-01 (3.2119e-01)\n",
      "Epoch: [9][178/707]\tTime  0.334 ( 0.345)\tData  0.199 ( 0.211)\tAcc@1  87.50 ( 82.16)\tAcc@5 100.00 (100.00)\tLoss 5.5264e-01 (4.9783e-01)\n",
      "Epoch: [9][355/707]\tTime  0.336 ( 0.342)\tData  0.208 ( 0.209)\tAcc@1 100.00 ( 81.16)\tAcc@5 100.00 (100.00)\tLoss 2.4555e-01 (5.3214e-01)\n",
      "Epoch: [9][532/707]\tTime  0.344 ( 0.341)\tData  0.211 ( 0.209)\tAcc@1  87.50 ( 81.32)\tAcc@5 100.00 (100.00)\tLoss 3.3567e-01 (5.3859e-01)\n",
      "Epoch: [9][707/707]\tTime  0.338 ( 0.340)\tData  0.215 ( 0.209)\tAcc@1  75.00 ( 81.51)\tAcc@5 100.00 (100.00)\tLoss 5.2483e-01 (5.3968e-01)\n",
      "[[ 205  150   27   61   23]\n",
      " [  66 1167   42  139   29]\n",
      " [   9   40  566  141   17]\n",
      " [  18   82   66 2454   38]\n",
      " [   3   35   12   48  218]]\n",
      "Epoch 9 perf acc@1: 81.50636291503906, perf acc@5: 100.0\n",
      "Best perf acc@1: 81.50636291503906, perf acc@5: 100.0 at epoch 9\n",
      "Epoch: 10 / 100\n",
      "Epoch: [10][  1/707]\tTime  1.098 ( 1.098)\tData  0.947 ( 0.947)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.5025e-01 (3.5025e-01)\n",
      "Epoch: [10][178/707]\tTime  0.330 ( 0.344)\tData  0.203 ( 0.210)\tAcc@1 100.00 ( 81.67)\tAcc@5 100.00 (100.00)\tLoss 4.5023e-02 (5.4544e-01)\n",
      "Epoch: [10][355/707]\tTime  0.337 ( 0.341)\tData  0.208 ( 0.209)\tAcc@1  87.50 ( 80.42)\tAcc@5 100.00 (100.00)\tLoss 2.3418e-01 (5.6119e-01)\n",
      "Epoch: [10][532/707]\tTime  0.340 ( 0.341)\tData  0.212 ( 0.209)\tAcc@1 100.00 ( 81.93)\tAcc@5 100.00 (100.00)\tLoss 2.8111e-01 (5.2891e-01)\n",
      "Epoch: [10][707/707]\tTime  0.334 ( 0.340)\tData  0.212 ( 0.208)\tAcc@1  87.50 ( 81.56)\tAcc@5 100.00 (100.00)\tLoss 2.8446e-01 (5.3500e-01)\n",
      "[[ 204  153   30   59   20]\n",
      " [  80 1160   27  139   37]\n",
      " [   7   33  576  147   10]\n",
      " [  11   88   76 2459   24]\n",
      " [   9   34   13   46  214]]\n",
      "Epoch 10 perf acc@1: 81.55941009521484, perf acc@5: 100.0\n",
      "Best perf acc@1: 81.55941009521484, perf acc@5: 100.0 at epoch 10\n",
      "Decreasing learning rate to 0.00590\n",
      "Epoch: 11 / 100\n",
      "Epoch: [11][  1/707]\tTime  1.203 ( 1.203)\tData  1.042 ( 1.042)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 5.4965e-02 (5.4965e-02)\n",
      "Epoch: [11][178/707]\tTime  0.344 ( 0.344)\tData  0.202 ( 0.212)\tAcc@1  87.50 ( 82.58)\tAcc@5 100.00 (100.00)\tLoss 4.0981e-01 (5.1170e-01)\n",
      "Epoch: [11][355/707]\tTime  0.340 ( 0.341)\tData  0.206 ( 0.210)\tAcc@1 100.00 ( 82.71)\tAcc@5 100.00 (100.00)\tLoss 1.2230e-01 (5.2547e-01)\n",
      "Epoch: [11][532/707]\tTime  0.339 ( 0.341)\tData  0.199 ( 0.209)\tAcc@1 100.00 ( 82.75)\tAcc@5 100.00 (100.00)\tLoss 2.1302e-01 (5.1450e-01)\n",
      "Epoch: [11][707/707]\tTime  0.335 ( 0.340)\tData  0.213 ( 0.209)\tAcc@1  87.50 ( 82.78)\tAcc@5 100.00 (100.00)\tLoss 2.6142e-01 (5.0894e-01)\n",
      "[[ 213  157   27   51   18]\n",
      " [  68 1187   28  134   26]\n",
      " [  10   38  582  130   13]\n",
      " [  11   79   65 2471   32]\n",
      " [   4   33    8   42  229]]\n",
      "Epoch 11 perf acc@1: 82.77935028076172, perf acc@5: 100.0\n",
      "Best perf acc@1: 82.77935028076172, perf acc@5: 100.0 at epoch 11\n",
      "Epoch: 12 / 100\n",
      "Epoch: [12][  1/707]\tTime  1.173 ( 1.173)\tData  1.022 ( 1.022)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 6.1240e-01 (6.1240e-01)\n",
      "Epoch: [12][178/707]\tTime  0.336 ( 0.344)\tData  0.208 ( 0.211)\tAcc@1 100.00 ( 84.76)\tAcc@5 100.00 (100.00)\tLoss 2.7569e-01 (4.4356e-01)\n",
      "Epoch: [12][355/707]\tTime  0.334 ( 0.342)\tData  0.206 ( 0.209)\tAcc@1  75.00 ( 84.15)\tAcc@5 100.00 (100.00)\tLoss 4.2585e-01 (4.7382e-01)\n",
      "Epoch: [12][532/707]\tTime  0.318 ( 0.341)\tData  0.189 ( 0.209)\tAcc@1  62.50 ( 83.79)\tAcc@5 100.00 (100.00)\tLoss 1.3437e+00 (4.7958e-01)\n",
      "Epoch: [12][707/707]\tTime  0.339 ( 0.340)\tData  0.215 ( 0.208)\tAcc@1 100.00 ( 83.65)\tAcc@5 100.00 (100.00)\tLoss 9.6091e-02 (4.8276e-01)\n",
      "[[ 241  132   26   50   17]\n",
      " [  68 1198   33  115   29]\n",
      " [  12   41  581  129   10]\n",
      " [  11   67   76 2477   27]\n",
      " [   7   35    5   35  234]]\n",
      "Epoch 12 perf acc@1: 83.64569091796875, perf acc@5: 100.0\n",
      "Best perf acc@1: 83.64569091796875, perf acc@5: 100.0 at epoch 12\n",
      "Decreasing learning rate to 0.00531\n",
      "Epoch: 13 / 100\n",
      "Epoch: [13][  1/707]\tTime  1.164 ( 1.164)\tData  1.011 ( 1.011)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 4.4855e-01 (4.4855e-01)\n",
      "Epoch: [13][178/707]\tTime  0.341 ( 0.344)\tData  0.209 ( 0.212)\tAcc@1  50.00 ( 83.08)\tAcc@5 100.00 (100.00)\tLoss 8.7810e-01 (4.8158e-01)\n",
      "Epoch: [13][355/707]\tTime  0.341 ( 0.342)\tData  0.208 ( 0.210)\tAcc@1  87.50 ( 84.37)\tAcc@5 100.00 (100.00)\tLoss 3.0758e-01 (4.6583e-01)\n",
      "Epoch: [13][532/707]\tTime  0.322 ( 0.341)\tData  0.194 ( 0.209)\tAcc@1 100.00 ( 84.73)\tAcc@5 100.00 (100.00)\tLoss 1.8146e-01 (4.6736e-01)\n",
      "Epoch: [13][707/707]\tTime  0.337 ( 0.340)\tData  0.215 ( 0.209)\tAcc@1  75.00 ( 84.23)\tAcc@5 100.00 (100.00)\tLoss 5.6435e-01 (4.7770e-01)\n",
      "[[ 230  133   27   58   18]\n",
      " [  67 1204   31  116   25]\n",
      " [  15   32  598  120    8]\n",
      " [  13   64   59 2491   31]\n",
      " [   5   25    6   39  241]]\n",
      "Epoch 13 perf acc@1: 84.22914123535156, perf acc@5: 100.0\n",
      "Best perf acc@1: 84.22914123535156, perf acc@5: 100.0 at epoch 13\n",
      "Epoch: 14 / 100\n",
      "Epoch: [14][  1/707]\tTime  1.247 ( 1.247)\tData  1.106 ( 1.106)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 6.4288e-01 (6.4288e-01)\n",
      "Epoch: [14][178/707]\tTime  0.355 ( 0.345)\tData  0.217 ( 0.212)\tAcc@1  75.00 ( 84.76)\tAcc@5 100.00 (100.00)\tLoss 1.0019e+00 (4.6461e-01)\n",
      "Epoch: [14][355/707]\tTime  0.334 ( 0.342)\tData  0.203 ( 0.209)\tAcc@1 100.00 ( 84.19)\tAcc@5 100.00 (100.00)\tLoss 9.9717e-02 (4.8358e-01)\n",
      "Epoch: [14][532/707]\tTime  0.334 ( 0.341)\tData  0.205 ( 0.208)\tAcc@1  87.50 ( 84.54)\tAcc@5 100.00 (100.00)\tLoss 5.3064e-01 (4.6720e-01)\n",
      "Epoch: [14][707/707]\tTime  0.334 ( 0.341)\tData  0.209 ( 0.208)\tAcc@1 100.00 ( 84.81)\tAcc@5 100.00 (100.00)\tLoss 5.8961e-02 (4.6068e-01)\n",
      "[[ 246  129   24   48   19]\n",
      " [  67 1221   33   98   24]\n",
      " [   8   36  595  123   11]\n",
      " [  12   64   60 2492   30]\n",
      " [   7   24    8   34  243]]\n",
      "Epoch 14 perf acc@1: 84.81259155273438, perf acc@5: 100.0\n",
      "Best perf acc@1: 84.81259155273438, perf acc@5: 100.0 at epoch 14\n",
      "Decreasing learning rate to 0.00478\n",
      "Epoch: 15 / 100\n",
      "Epoch: [15][  1/707]\tTime  1.095 ( 1.095)\tData  0.933 ( 0.933)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.4079e-01 (1.4079e-01)\n",
      "Epoch: [15][178/707]\tTime  0.332 ( 0.344)\tData  0.206 ( 0.211)\tAcc@1  62.50 ( 84.34)\tAcc@5 100.00 (100.00)\tLoss 5.3879e-01 (4.5593e-01)\n",
      "Epoch: [15][355/707]\tTime  0.339 ( 0.341)\tData  0.208 ( 0.209)\tAcc@1  87.50 ( 84.89)\tAcc@5 100.00 (100.00)\tLoss 3.9285e-01 (4.4875e-01)\n",
      "Epoch: [15][532/707]\tTime  0.334 ( 0.340)\tData  0.205 ( 0.209)\tAcc@1  62.50 ( 84.84)\tAcc@5 100.00 (100.00)\tLoss 9.0719e-01 (4.4911e-01)\n",
      "Epoch: [15][707/707]\tTime  0.335 ( 0.340)\tData  0.212 ( 0.208)\tAcc@1  62.50 ( 85.31)\tAcc@5 100.00 (100.00)\tLoss 1.2283e+00 (4.3837e-01)\n",
      "[[ 245  134   28   42   17]\n",
      " [  63 1223   28  106   23]\n",
      " [  11   35  606  114    7]\n",
      " [  10   62   51 2506   29]\n",
      " [   2   29    5   35  245]]\n",
      "Epoch 15 perf acc@1: 85.3076400756836, perf acc@5: 100.0\n",
      "Best perf acc@1: 85.3076400756836, perf acc@5: 100.0 at epoch 15\n",
      "Epoch: 16 / 100\n",
      "Epoch: [16][  1/707]\tTime  1.305 ( 1.305)\tData  1.151 ( 1.151)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.4271e-01 (3.4271e-01)\n",
      "Epoch: [16][178/707]\tTime  0.331 ( 0.344)\tData  0.203 ( 0.212)\tAcc@1  62.50 ( 84.62)\tAcc@5 100.00 (100.00)\tLoss 8.5935e-01 (4.3084e-01)\n",
      "Epoch: [16][355/707]\tTime  0.341 ( 0.341)\tData  0.209 ( 0.210)\tAcc@1  87.50 ( 86.30)\tAcc@5 100.00 (100.00)\tLoss 2.0019e-01 (4.0451e-01)\n",
      "Epoch: [16][532/707]\tTime  0.333 ( 0.341)\tData  0.205 ( 0.209)\tAcc@1  75.00 ( 85.67)\tAcc@5 100.00 (100.00)\tLoss 7.0667e-01 (4.2565e-01)\n",
      "Epoch: [16][707/707]\tTime  0.336 ( 0.340)\tData  0.212 ( 0.209)\tAcc@1  87.50 ( 85.10)\tAcc@5 100.00 (100.00)\tLoss 2.3616e-01 (4.3809e-01)\n",
      "[[ 256  133   20   39   18]\n",
      " [  68 1229   30   94   22]\n",
      " [  11   35  597  123    7]\n",
      " [   7   69   67 2483   32]\n",
      " [   8   27    7   26  248]]\n",
      "Epoch 16 perf acc@1: 85.09547424316406, perf acc@5: 100.0\n",
      "Best perf acc@1: 85.3076400756836, perf acc@5: 100.0 at epoch 15\n",
      "Decreasing learning rate to 0.00430\n",
      "Epoch: 17 / 100\n",
      "Epoch: [17][  1/707]\tTime  1.067 ( 1.067)\tData  0.904 ( 0.904)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 6.4648e-01 (6.4648e-01)\n",
      "Epoch: [17][178/707]\tTime  0.332 ( 0.344)\tData  0.203 ( 0.210)\tAcc@1  62.50 ( 85.96)\tAcc@5 100.00 (100.00)\tLoss 6.9730e-01 (4.2804e-01)\n",
      "Epoch: [17][355/707]\tTime  0.333 ( 0.342)\tData  0.186 ( 0.209)\tAcc@1  87.50 ( 86.51)\tAcc@5 100.00 (100.00)\tLoss 5.2239e-01 (4.0655e-01)\n",
      "Epoch: [17][532/707]\tTime  0.342 ( 0.341)\tData  0.212 ( 0.208)\tAcc@1  87.50 ( 86.54)\tAcc@5 100.00 (100.00)\tLoss 2.4315e-01 (4.0856e-01)\n",
      "Epoch: [17][707/707]\tTime  0.336 ( 0.340)\tData  0.212 ( 0.208)\tAcc@1  75.00 ( 86.47)\tAcc@5 100.00 (100.00)\tLoss 3.9215e-01 (4.1921e-01)\n",
      "[[ 268  114   21   44   19]\n",
      " [  68 1232   26   90   27]\n",
      " [  11   25  619  110    8]\n",
      " [  12   56   47 2518   25]\n",
      " [   5   22    9   26  254]]\n",
      "Epoch 17 perf acc@1: 86.47454071044922, perf acc@5: 100.0\n",
      "Best perf acc@1: 86.47454071044922, perf acc@5: 100.0 at epoch 17\n",
      "Epoch: 18 / 100\n",
      "Epoch: [18][  1/707]\tTime  1.205 ( 1.205)\tData  1.070 ( 1.070)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 9.0886e-02 (9.0886e-02)\n",
      "Epoch: [18][178/707]\tTime  0.346 ( 0.344)\tData  0.206 ( 0.212)\tAcc@1 100.00 ( 86.66)\tAcc@5 100.00 (100.00)\tLoss 1.3219e-01 (3.9474e-01)\n",
      "Epoch: [18][355/707]\tTime  0.334 ( 0.341)\tData  0.202 ( 0.210)\tAcc@1  62.50 ( 86.44)\tAcc@5 100.00 (100.00)\tLoss 5.4979e-01 (4.0995e-01)\n",
      "Epoch: [18][532/707]\tTime  0.347 ( 0.341)\tData  0.219 ( 0.209)\tAcc@1  87.50 ( 86.72)\tAcc@5 100.00 (100.00)\tLoss 3.9631e-01 (4.0587e-01)\n",
      "Epoch: [18][707/707]\tTime  0.330 ( 0.340)\tData  0.208 ( 0.209)\tAcc@1 100.00 ( 86.76)\tAcc@5 100.00 (100.00)\tLoss 1.2478e-01 (4.0622e-01)\n",
      "[[ 272  112   26   44   12]\n",
      " [  66 1241   27   89   20]\n",
      " [  10   33  606  116    8]\n",
      " [  13   58   55 2517   15]\n",
      " [   7   16    6   16  271]]\n",
      "Epoch 18 perf acc@1: 86.75743103027344, perf acc@5: 100.0\n",
      "Best perf acc@1: 86.75743103027344, perf acc@5: 100.0 at epoch 18\n",
      "Decreasing learning rate to 0.00387\n",
      "Epoch: 19 / 100\n",
      "Epoch: [19][  1/707]\tTime  1.032 ( 1.032)\tData  0.893 ( 0.893)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.0341e-01 (3.0341e-01)\n",
      "Epoch: [19][178/707]\tTime  0.337 ( 0.343)\tData  0.208 ( 0.211)\tAcc@1  75.00 ( 88.76)\tAcc@5 100.00 (100.00)\tLoss 1.0992e+00 (3.6567e-01)\n",
      "Epoch: [19][355/707]\tTime  0.332 ( 0.341)\tData  0.204 ( 0.209)\tAcc@1  75.00 ( 87.25)\tAcc@5 100.00 (100.00)\tLoss 6.7089e-01 (3.9215e-01)\n",
      "Epoch: [19][532/707]\tTime  0.337 ( 0.340)\tData  0.205 ( 0.209)\tAcc@1  87.50 ( 87.31)\tAcc@5 100.00 (100.00)\tLoss 3.1127e-01 (3.9128e-01)\n",
      "Epoch: [19][707/707]\tTime  0.333 ( 0.340)\tData  0.212 ( 0.209)\tAcc@1  87.50 ( 87.08)\tAcc@5 100.00 (100.00)\tLoss 3.8814e-01 (3.9579e-01)\n",
      "[[ 267  118   20   48   13]\n",
      " [  60 1243   32   88   20]\n",
      " [   9   26  631   99    8]\n",
      " [   8   63   40 2525   22]\n",
      " [   4   15    6   32  259]]\n",
      "Epoch 19 perf acc@1: 87.07567596435547, perf acc@5: 100.0\n",
      "Best perf acc@1: 87.07567596435547, perf acc@5: 100.0 at epoch 19\n",
      "Epoch: 20 / 100\n",
      "Epoch: [20][  1/707]\tTime  1.208 ( 1.208)\tData  1.068 ( 1.068)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.2088e-01 (1.2088e-01)\n",
      "Epoch: [20][178/707]\tTime  0.338 ( 0.344)\tData  0.206 ( 0.212)\tAcc@1  87.50 ( 87.36)\tAcc@5 100.00 (100.00)\tLoss 6.6770e-01 (3.9347e-01)\n",
      "Epoch: [20][355/707]\tTime  0.338 ( 0.341)\tData  0.206 ( 0.209)\tAcc@1 100.00 ( 87.15)\tAcc@5 100.00 (100.00)\tLoss 3.2958e-02 (3.9114e-01)\n",
      "Epoch: [20][532/707]\tTime  0.348 ( 0.340)\tData  0.213 ( 0.209)\tAcc@1  75.00 ( 87.19)\tAcc@5 100.00 (100.00)\tLoss 7.3121e-01 (3.8707e-01)\n",
      "Epoch: [20][707/707]\tTime  0.337 ( 0.340)\tData  0.213 ( 0.208)\tAcc@1 100.00 ( 87.15)\tAcc@5 100.00 (100.00)\tLoss 1.2417e-01 (3.9101e-01)\n",
      "[[ 276  107   24   41   18]\n",
      " [  60 1242   26   96   19]\n",
      " [   9   31  626  100    7]\n",
      " [   9   49   56 2524   20]\n",
      " [   4   23    6   22  261]]\n",
      "Epoch 20 perf acc@1: 87.14639282226562, perf acc@5: 100.0\n",
      "Best perf acc@1: 87.14639282226562, perf acc@5: 100.0 at epoch 20\n",
      "Decreasing learning rate to 0.00349\n",
      "Epoch: 21 / 100\n",
      "Epoch: [21][  1/707]\tTime  1.267 ( 1.267)\tData  1.103 ( 1.103)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.1575e-01 (3.1575e-01)\n",
      "Epoch: [21][178/707]\tTime  0.340 ( 0.345)\tData  0.210 ( 0.213)\tAcc@1  87.50 ( 85.74)\tAcc@5 100.00 (100.00)\tLoss 1.9696e-01 (4.1839e-01)\n",
      "Epoch: [21][355/707]\tTime  0.341 ( 0.341)\tData  0.211 ( 0.210)\tAcc@1  75.00 ( 86.80)\tAcc@5 100.00 (100.00)\tLoss 4.2970e-01 (3.9089e-01)\n",
      "Epoch: [21][532/707]\tTime  0.339 ( 0.341)\tData  0.210 ( 0.209)\tAcc@1 100.00 ( 87.27)\tAcc@5 100.00 (100.00)\tLoss 8.1507e-02 (3.7761e-01)\n",
      "Epoch: [21][707/707]\tTime  0.333 ( 0.340)\tData  0.211 ( 0.209)\tAcc@1 100.00 ( 87.48)\tAcc@5 100.00 (100.00)\tLoss 5.4492e-02 (3.7120e-01)\n",
      "[[ 279  112   20   37   18]\n",
      " [  63 1255   23   88   14]\n",
      " [  12   31  627   96    7]\n",
      " [  10   58   48 2524   18]\n",
      " [   5   20    4   24  263]]\n",
      "Epoch 21 perf acc@1: 87.4823226928711, perf acc@5: 100.0\n",
      "Best perf acc@1: 87.4823226928711, perf acc@5: 100.0 at epoch 21\n",
      "Epoch: 22 / 100\n",
      "Epoch: [22][  1/707]\tTime  1.260 ( 1.260)\tData  1.121 ( 1.121)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.9280e-01 (1.9280e-01)\n",
      "Epoch: [22][178/707]\tTime  0.337 ( 0.344)\tData  0.209 ( 0.212)\tAcc@1  87.50 ( 87.92)\tAcc@5 100.00 (100.00)\tLoss 3.4213e-01 (3.5601e-01)\n",
      "Epoch: [22][355/707]\tTime  0.337 ( 0.341)\tData  0.210 ( 0.210)\tAcc@1  87.50 ( 87.01)\tAcc@5 100.00 (100.00)\tLoss 3.0048e-01 (3.7067e-01)\n",
      "Epoch: [22][532/707]\tTime  0.344 ( 0.341)\tData  0.206 ( 0.209)\tAcc@1  87.50 ( 87.22)\tAcc@5 100.00 (100.00)\tLoss 4.4066e-01 (3.7233e-01)\n",
      "Epoch: [22][707/707]\tTime  0.333 ( 0.340)\tData  0.209 ( 0.209)\tAcc@1 100.00 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.0217e-01 (3.6725e-01)\n",
      "[[ 275  107   18   47   19]\n",
      " [  67 1256   27   72   21]\n",
      " [  12   29  635   91    6]\n",
      " [  10   59   43 2524   22]\n",
      " [   7   16   10   24  259]]\n",
      "Epoch 22 perf acc@1: 87.5, perf acc@5: 100.0\n",
      "Best perf acc@1: 87.5, perf acc@5: 100.0 at epoch 22\n",
      "Decreasing learning rate to 0.00314\n",
      "Epoch: 23 / 100\n",
      "Epoch: [23][  1/707]\tTime  1.259 ( 1.259)\tData  1.113 ( 1.113)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 1.6969e-01 (1.6969e-01)\n",
      "Epoch: [23][178/707]\tTime  0.339 ( 0.345)\tData  0.207 ( 0.213)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.2339e-01 (3.5565e-01)\n",
      "Epoch: [23][355/707]\tTime  0.344 ( 0.342)\tData  0.211 ( 0.210)\tAcc@1 100.00 ( 87.43)\tAcc@5 100.00 (100.00)\tLoss 1.8044e-01 (3.5733e-01)\n",
      "Epoch: [23][532/707]\tTime  0.339 ( 0.341)\tData  0.207 ( 0.209)\tAcc@1  87.50 ( 87.52)\tAcc@5 100.00 (100.00)\tLoss 5.4410e-01 (3.6316e-01)\n",
      "Epoch: [23][707/707]\tTime  0.335 ( 0.340)\tData  0.211 ( 0.209)\tAcc@1 100.00 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.1766e-02 (3.6330e-01)\n",
      "[[ 291  102   20   37   16]\n",
      " [  59 1248   24   90   22]\n",
      " [   9   23  629  103    9]\n",
      " [  15   51   53 2521   18]\n",
      " [   7   22    6   21  260]]\n",
      "Epoch 23 perf acc@1: 87.5, perf acc@5: 100.0\n",
      "Best perf acc@1: 87.5, perf acc@5: 100.0 at epoch 23\n",
      "Epoch: 24 / 100\n",
      "Epoch: [24][  1/707]\tTime  1.156 ( 1.156)\tData  0.994 ( 0.994)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.4733e-01 (2.4733e-01)\n",
      "Epoch: [24][178/707]\tTime  0.355 ( 0.344)\tData  0.199 ( 0.210)\tAcc@1 100.00 ( 88.62)\tAcc@5 100.00 (100.00)\tLoss 1.3109e-01 (3.3719e-01)\n",
      "Epoch: [24][355/707]\tTime  0.346 ( 0.342)\tData  0.211 ( 0.209)\tAcc@1  62.50 ( 87.54)\tAcc@5 100.00 (100.00)\tLoss 1.0668e+00 (3.7146e-01)\n",
      "Epoch: [24][532/707]\tTime  0.334 ( 0.341)\tData  0.202 ( 0.208)\tAcc@1  87.50 ( 87.45)\tAcc@5 100.00 (100.00)\tLoss 3.7987e-01 (3.6957e-01)\n",
      "Epoch: [24][707/707]\tTime  0.334 ( 0.340)\tData  0.210 ( 0.208)\tAcc@1  87.50 ( 87.31)\tAcc@5 100.00 (100.00)\tLoss 3.2050e-01 (3.7017e-01)\n",
      "[[ 275  107   24   46   14]\n",
      " [  63 1240   28   89   23]\n",
      " [  13   28  616  109    7]\n",
      " [  12   55   39 2536   16]\n",
      " [   5   20    5   15  271]]\n",
      "Epoch 24 perf acc@1: 87.3055191040039, perf acc@5: 100.0\n",
      "Best perf acc@1: 87.5, perf acc@5: 100.0 at epoch 23\n",
      "Decreasing learning rate to 0.00282\n",
      "Epoch: 25 / 100\n",
      "Epoch: [25][  1/707]\tTime  1.044 ( 1.044)\tData  0.867 ( 0.867)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 4.7987e-01 (4.7987e-01)\n",
      "Epoch: [25][178/707]\tTime  0.332 ( 0.344)\tData  0.205 ( 0.210)\tAcc@1 100.00 ( 88.34)\tAcc@5 100.00 (100.00)\tLoss 9.1701e-02 (3.4437e-01)\n",
      "Epoch: [25][355/707]\tTime  0.342 ( 0.342)\tData  0.200 ( 0.209)\tAcc@1  62.50 ( 88.35)\tAcc@5 100.00 (100.00)\tLoss 9.4042e-01 (3.4249e-01)\n",
      "Epoch: [25][532/707]\tTime  0.344 ( 0.341)\tData  0.217 ( 0.209)\tAcc@1  87.50 ( 88.58)\tAcc@5 100.00 (100.00)\tLoss 3.8226e-01 (3.4390e-01)\n",
      "Epoch: [25][707/707]\tTime  0.335 ( 0.340)\tData  0.212 ( 0.209)\tAcc@1 100.00 ( 88.40)\tAcc@5 100.00 (100.00)\tLoss 8.1051e-02 (3.4904e-01)\n",
      "[[ 288  110   20   38   10]\n",
      " [  57 1272   28   75   11]\n",
      " [   5   26  637   96    9]\n",
      " [  14   42   50 2529   23]\n",
      " [   5   13    5   19  274]]\n",
      "Epoch 25 perf acc@1: 88.40169525146484, perf acc@5: 100.0\n",
      "Best perf acc@1: 88.40169525146484, perf acc@5: 100.0 at epoch 25\n",
      "Epoch: 26 / 100\n",
      "Epoch: [26][  1/707]\tTime  1.318 ( 1.318)\tData  1.172 ( 1.172)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.1905e-01 (3.1905e-01)\n",
      "Epoch: [26][178/707]\tTime  0.336 ( 0.345)\tData  0.203 ( 0.212)\tAcc@1  87.50 ( 89.40)\tAcc@5 100.00 (100.00)\tLoss 3.0708e-01 (3.2987e-01)\n",
      "Epoch: [26][355/707]\tTime  0.338 ( 0.342)\tData  0.209 ( 0.210)\tAcc@1  75.00 ( 89.54)\tAcc@5 100.00 (100.00)\tLoss 1.2028e+00 (3.2514e-01)\n",
      "Epoch: [26][532/707]\tTime  0.338 ( 0.341)\tData  0.208 ( 0.209)\tAcc@1  62.50 ( 89.07)\tAcc@5 100.00 (100.00)\tLoss 5.6304e-01 (3.3662e-01)\n",
      "Epoch: [26][707/707]\tTime  0.335 ( 0.341)\tData  0.214 ( 0.209)\tAcc@1 100.00 ( 89.18)\tAcc@5 100.00 (100.00)\tLoss 7.8780e-02 (3.3389e-01)\n",
      "[[ 292  102   20   42   10]\n",
      " [  59 1272   30   70   12]\n",
      " [   8   25  660   73    7]\n",
      " [  13   48   43 2543   11]\n",
      " [   6   13    4   16  277]]\n",
      "Epoch 26 perf acc@1: 89.17963409423828, perf acc@5: 100.0\n",
      "Best perf acc@1: 89.17963409423828, perf acc@5: 100.0 at epoch 26\n",
      "Decreasing learning rate to 0.00254\n",
      "Epoch: 27 / 100\n",
      "Epoch: [27][  1/707]\tTime  1.231 ( 1.231)\tData  1.077 ( 1.077)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 8.4612e-01 (8.4612e-01)\n",
      "Epoch: [27][178/707]\tTime  0.340 ( 0.345)\tData  0.210 ( 0.213)\tAcc@1  75.00 ( 90.10)\tAcc@5 100.00 (100.00)\tLoss 3.8920e-01 (3.1690e-01)\n",
      "Epoch: [27][355/707]\tTime  0.336 ( 0.342)\tData  0.206 ( 0.210)\tAcc@1  87.50 ( 89.40)\tAcc@5 100.00 (100.00)\tLoss 6.9586e-01 (3.2761e-01)\n",
      "Epoch: [27][532/707]\tTime  0.335 ( 0.341)\tData  0.208 ( 0.209)\tAcc@1 100.00 ( 88.77)\tAcc@5 100.00 (100.00)\tLoss 2.4049e-01 (3.3493e-01)\n",
      "Epoch: [27][707/707]\tTime  0.342 ( 0.340)\tData  0.214 ( 0.209)\tAcc@1 100.00 ( 88.99)\tAcc@5 100.00 (100.00)\tLoss 1.1779e-01 (3.3275e-01)\n",
      "[[ 303   93   20   35   15]\n",
      " [  48 1276   25   78   16]\n",
      " [  14   23  644   83    9]\n",
      " [   8   65   42 2533   10]\n",
      " [   9    9    8   13  277]]\n",
      "Epoch 27 perf acc@1: 88.98515319824219, perf acc@5: 100.0\n",
      "Best perf acc@1: 89.17963409423828, perf acc@5: 100.0 at epoch 26\n",
      "Epoch: 28 / 100\n",
      "Epoch: [28][  1/707]\tTime  1.136 ( 1.136)\tData  0.984 ( 0.984)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.1804e-01 (1.1804e-01)\n",
      "Epoch: [28][178/707]\tTime  0.333 ( 0.344)\tData  0.204 ( 0.211)\tAcc@1  87.50 ( 89.40)\tAcc@5 100.00 (100.00)\tLoss 4.7448e-01 (3.2079e-01)\n",
      "Epoch: [28][355/707]\tTime  0.332 ( 0.342)\tData  0.204 ( 0.209)\tAcc@1 100.00 ( 88.45)\tAcc@5 100.00 (100.00)\tLoss 6.2378e-02 (3.3064e-01)\n",
      "Epoch: [28][532/707]\tTime  0.339 ( 0.341)\tData  0.210 ( 0.209)\tAcc@1  75.00 ( 88.79)\tAcc@5 100.00 (100.00)\tLoss 5.8800e-01 (3.2411e-01)\n",
      "Epoch: [28][707/707]\tTime  0.338 ( 0.340)\tData  0.215 ( 0.209)\tAcc@1  87.50 ( 88.51)\tAcc@5 100.00 (100.00)\tLoss 3.1710e-01 (3.2702e-01)\n",
      "[[ 309   89   20   38   10]\n",
      " [  54 1261   22   88   18]\n",
      " [  10   30  633   95    5]\n",
      " [  14   45   55 2529   15]\n",
      " [   5   14    4   19  274]]\n",
      "Epoch 28 perf acc@1: 88.50778198242188, perf acc@5: 100.0\n",
      "Best perf acc@1: 89.17963409423828, perf acc@5: 100.0 at epoch 26\n",
      "Decreasing learning rate to 0.00229\n",
      "Epoch: 29 / 100\n",
      "Epoch: [29][  1/707]\tTime  0.902 ( 0.902)\tData  0.756 ( 0.756)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.7530e-01 (1.7530e-01)\n",
      "Epoch: [29][178/707]\tTime  0.351 ( 0.343)\tData  0.221 ( 0.210)\tAcc@1 100.00 ( 89.12)\tAcc@5 100.00 (100.00)\tLoss 1.7849e-01 (3.3623e-01)\n",
      "Epoch: [29][355/707]\tTime  0.334 ( 0.340)\tData  0.208 ( 0.208)\tAcc@1  87.50 ( 89.30)\tAcc@5 100.00 (100.00)\tLoss 1.5138e-01 (3.2320e-01)\n",
      "Epoch: [29][532/707]\tTime  0.346 ( 0.340)\tData  0.211 ( 0.208)\tAcc@1  62.50 ( 89.07)\tAcc@5 100.00 (100.00)\tLoss 9.1354e-01 (3.1879e-01)\n",
      "Epoch: [29][707/707]\tTime  0.335 ( 0.340)\tData  0.211 ( 0.208)\tAcc@1  62.50 ( 88.60)\tAcc@5 100.00 (100.00)\tLoss 5.8155e-01 (3.2809e-01)\n",
      "[[ 299   93   21   34   19]\n",
      " [  58 1277   21   77   10]\n",
      " [   8   25  636   96    8]\n",
      " [  12   49   50 2526   21]\n",
      " [   6   13    4   20  273]]\n",
      "Epoch 29 perf acc@1: 88.59618377685547, perf acc@5: 100.0\n",
      "Best perf acc@1: 89.17963409423828, perf acc@5: 100.0 at epoch 26\n",
      "Epoch: 30 / 100\n",
      "Epoch: [30][  1/707]\tTime  1.111 ( 1.111)\tData  0.961 ( 0.961)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 7.9303e-02 (7.9303e-02)\n",
      "Epoch: [30][178/707]\tTime  0.342 ( 0.343)\tData  0.210 ( 0.211)\tAcc@1 100.00 ( 88.90)\tAcc@5 100.00 (100.00)\tLoss 8.0823e-02 (3.2188e-01)\n",
      "Epoch: [30][355/707]\tTime  0.338 ( 0.341)\tData  0.209 ( 0.209)\tAcc@1  87.50 ( 88.20)\tAcc@5 100.00 (100.00)\tLoss 8.3266e-01 (3.3923e-01)\n",
      "Epoch: [30][532/707]\tTime  0.332 ( 0.340)\tData  0.204 ( 0.208)\tAcc@1  87.50 ( 89.07)\tAcc@5 100.00 (100.00)\tLoss 3.5110e-01 (3.2217e-01)\n",
      "Epoch: [30][707/707]\tTime  0.338 ( 0.340)\tData  0.215 ( 0.208)\tAcc@1 100.00 ( 89.34)\tAcc@5 100.00 (100.00)\tLoss 1.6094e-01 (3.1781e-01)\n",
      "[[ 307   98   16   35   10]\n",
      " [  55 1282   24   67   15]\n",
      " [   8   24  638   98    5]\n",
      " [  11   45   43 2548   11]\n",
      " [   3   12    6   17  278]]\n",
      "Epoch 30 perf acc@1: 89.33876037597656, perf acc@5: 100.0\n",
      "Best perf acc@1: 89.33876037597656, perf acc@5: 100.0 at epoch 30\n",
      "Decreasing learning rate to 0.00206\n",
      "Epoch: 31 / 100\n",
      "Epoch: [31][  1/707]\tTime  1.191 ( 1.191)\tData  1.041 ( 1.041)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.8463e-02 (4.8463e-02)\n",
      "Epoch: [31][178/707]\tTime  0.331 ( 0.344)\tData  0.204 ( 0.212)\tAcc@1 100.00 ( 91.36)\tAcc@5 100.00 (100.00)\tLoss 7.9287e-02 (2.6956e-01)\n",
      "Epoch: [31][355/707]\tTime  0.344 ( 0.341)\tData  0.215 ( 0.210)\tAcc@1  75.00 ( 90.32)\tAcc@5 100.00 (100.00)\tLoss 8.5835e-01 (2.9312e-01)\n",
      "Epoch: [31][532/707]\tTime  0.345 ( 0.340)\tData  0.212 ( 0.209)\tAcc@1 100.00 ( 90.44)\tAcc@5 100.00 (100.00)\tLoss 1.1418e-01 (2.9010e-01)\n",
      "Epoch: [31][707/707]\tTime  0.338 ( 0.340)\tData  0.214 ( 0.209)\tAcc@1 100.00 ( 90.33)\tAcc@5 100.00 (100.00)\tLoss 2.3952e-02 (2.9046e-01)\n",
      "[[ 329   76   21   31    9]\n",
      " [  51 1287   26   66   13]\n",
      " [  11   31  651   76    4]\n",
      " [   9   41   44 2554   10]\n",
      " [   1    9    2   16  288]]\n",
      "Epoch 31 perf acc@1: 90.328857421875, perf acc@5: 100.0\n",
      "Best perf acc@1: 90.328857421875, perf acc@5: 100.0 at epoch 31\n",
      "Epoch: 32 / 100\n",
      "Epoch: [32][  1/707]\tTime  1.094 ( 1.094)\tData  0.942 ( 0.942)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.3326e-01 (2.3326e-01)\n",
      "Epoch: [32][178/707]\tTime  0.345 ( 0.344)\tData  0.214 ( 0.211)\tAcc@1  87.50 ( 90.52)\tAcc@5 100.00 (100.00)\tLoss 1.7715e-01 (2.8420e-01)\n",
      "Epoch: [32][355/707]\tTime  0.354 ( 0.341)\tData  0.207 ( 0.210)\tAcc@1  75.00 ( 91.13)\tAcc@5 100.00 (100.00)\tLoss 3.8737e-01 (2.7389e-01)\n",
      "Epoch: [32][532/707]\tTime  0.339 ( 0.340)\tData  0.210 ( 0.209)\tAcc@1 100.00 ( 90.72)\tAcc@5 100.00 (100.00)\tLoss 1.3371e-01 (2.8194e-01)\n",
      "Epoch: [32][707/707]\tTime  0.334 ( 0.340)\tData  0.212 ( 0.209)\tAcc@1  75.00 ( 90.45)\tAcc@5 100.00 (100.00)\tLoss 4.5939e-01 (2.9222e-01)\n",
      "[[ 325   75   19   36   11]\n",
      " [  50 1285   23   75   10]\n",
      " [   5   20  660   83    5]\n",
      " [  10   50   33 2557    8]\n",
      " [   1   10    5   11  289]]\n",
      "Epoch 32 perf acc@1: 90.45262145996094, perf acc@5: 100.0\n",
      "Best perf acc@1: 90.45262145996094, perf acc@5: 100.0 at epoch 32\n",
      "Decreasing learning rate to 0.00185\n",
      "Epoch: 33 / 100\n",
      "Epoch: [33][  1/707]\tTime  1.191 ( 1.191)\tData  1.047 ( 1.047)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 6.8178e-01 (6.8178e-01)\n",
      "Epoch: [33][178/707]\tTime  0.326 ( 0.344)\tData  0.196 ( 0.212)\tAcc@1 100.00 ( 89.89)\tAcc@5 100.00 (100.00)\tLoss 5.1267e-02 (2.9779e-01)\n",
      "Epoch: [33][355/707]\tTime  0.339 ( 0.341)\tData  0.207 ( 0.210)\tAcc@1 100.00 ( 90.07)\tAcc@5 100.00 (100.00)\tLoss 1.3994e-01 (2.9563e-01)\n",
      "Epoch: [33][532/707]\tTime  0.336 ( 0.341)\tData  0.210 ( 0.209)\tAcc@1  87.50 ( 89.90)\tAcc@5 100.00 (100.00)\tLoss 2.0495e-01 (2.9669e-01)\n",
      "Epoch: [33][707/707]\tTime  0.343 ( 0.340)\tData  0.216 ( 0.209)\tAcc@1  87.50 ( 90.28)\tAcc@5 100.00 (100.00)\tLoss 2.8960e-01 (2.8673e-01)\n",
      "[[ 322   83   20   30   11]\n",
      " [  51 1295   26   60   11]\n",
      " [  11   27  652   74    9]\n",
      " [  15   26   46 2559   12]\n",
      " [   4    9    5   20  278]]\n",
      "Epoch 33 perf acc@1: 90.27581787109375, perf acc@5: 100.0\n",
      "Best perf acc@1: 90.45262145996094, perf acc@5: 100.0 at epoch 32\n",
      "Epoch: 34 / 100\n",
      "Epoch: [34][  1/707]\tTime  1.060 ( 1.060)\tData  0.911 ( 0.911)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.3510e-01 (1.3510e-01)\n",
      "Epoch: [34][178/707]\tTime  0.337 ( 0.343)\tData  0.209 ( 0.211)\tAcc@1  87.50 ( 91.50)\tAcc@5 100.00 (100.00)\tLoss 2.5838e-01 (2.6224e-01)\n",
      "Epoch: [34][355/707]\tTime  0.335 ( 0.341)\tData  0.204 ( 0.209)\tAcc@1 100.00 ( 91.09)\tAcc@5 100.00 (100.00)\tLoss 3.8790e-02 (2.7232e-01)\n",
      "Epoch: [34][532/707]\tTime  0.332 ( 0.340)\tData  0.204 ( 0.209)\tAcc@1 100.00 ( 90.72)\tAcc@5 100.00 (100.00)\tLoss 1.0194e-01 (2.7845e-01)\n",
      "Epoch: [34][707/707]\tTime  0.335 ( 0.340)\tData  0.213 ( 0.208)\tAcc@1  75.00 ( 90.81)\tAcc@5 100.00 (100.00)\tLoss 5.0161e-01 (2.7576e-01)\n",
      "[[ 333   69   18   35   11]\n",
      " [  52 1288   28   63   12]\n",
      " [   8   21  665   76    3]\n",
      " [  12   35   42 2559   10]\n",
      " [   2   11    4    8  291]]\n",
      "Epoch 34 perf acc@1: 90.80622863769531, perf acc@5: 100.0\n",
      "Best perf acc@1: 90.80622863769531, perf acc@5: 100.0 at epoch 34\n",
      "Decreasing learning rate to 0.00167\n",
      "Epoch: 35 / 100\n",
      "Epoch: [35][  1/707]\tTime  1.313 ( 1.313)\tData  1.158 ( 1.158)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 4.7281e-01 (4.7281e-01)\n",
      "Epoch: [35][178/707]\tTime  0.337 ( 0.344)\tData  0.210 ( 0.212)\tAcc@1  87.50 ( 90.03)\tAcc@5 100.00 (100.00)\tLoss 2.5215e-01 (2.9133e-01)\n",
      "Epoch: [35][355/707]\tTime  0.334 ( 0.341)\tData  0.206 ( 0.210)\tAcc@1 100.00 ( 90.39)\tAcc@5 100.00 (100.00)\tLoss 7.2848e-02 (2.8322e-01)\n",
      "Epoch: [35][532/707]\tTime  0.336 ( 0.340)\tData  0.207 ( 0.209)\tAcc@1  87.50 ( 90.51)\tAcc@5 100.00 (100.00)\tLoss 1.6690e-01 (2.7655e-01)\n",
      "Epoch: [35][707/707]\tTime  0.334 ( 0.340)\tData  0.212 ( 0.208)\tAcc@1 100.00 ( 90.61)\tAcc@5 100.00 (100.00)\tLoss 1.5740e-01 (2.8037e-01)\n",
      "[[ 323   85   16   35    7]\n",
      " [  44 1301   19   72    7]\n",
      " [   9   22  666   71    5]\n",
      " [   8   49   41 2547   13]\n",
      " [   6    9    5    8  288]]\n",
      "Epoch 35 perf acc@1: 90.61174011230469, perf acc@5: 100.0\n",
      "Best perf acc@1: 90.80622863769531, perf acc@5: 100.0 at epoch 34\n",
      "Epoch: 36 / 100\n",
      "Epoch: [36][  1/707]\tTime  1.281 ( 1.281)\tData  1.131 ( 1.131)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.0508e-01 (2.0508e-01)\n",
      "Epoch: [36][178/707]\tTime  0.329 ( 0.344)\tData  0.196 ( 0.212)\tAcc@1  87.50 ( 90.38)\tAcc@5 100.00 (100.00)\tLoss 3.3753e-01 (2.6668e-01)\n",
      "Epoch: [36][355/707]\tTime  0.346 ( 0.341)\tData  0.204 ( 0.210)\tAcc@1  87.50 ( 91.51)\tAcc@5 100.00 (100.00)\tLoss 2.4854e-01 (2.4544e-01)\n",
      "Epoch: [36][532/707]\tTime  0.334 ( 0.340)\tData  0.207 ( 0.209)\tAcc@1 100.00 ( 91.49)\tAcc@5 100.00 (100.00)\tLoss 8.2799e-02 (2.5430e-01)\n",
      "Epoch: [36][707/707]\tTime  0.334 ( 0.340)\tData  0.212 ( 0.209)\tAcc@1  50.00 ( 90.98)\tAcc@5 100.00 (100.00)\tLoss 7.6437e-01 (2.6406e-01)\n",
      "[[ 329   82   18   30    7]\n",
      " [  50 1293   23   65   12]\n",
      " [   3   25  670   68    7]\n",
      " [   9   38   38 2564    9]\n",
      " [   2    8    5   11  290]]\n",
      "Epoch 36 perf acc@1: 90.9830322265625, perf acc@5: 100.0\n",
      "Best perf acc@1: 90.9830322265625, perf acc@5: 100.0 at epoch 36\n",
      "Decreasing learning rate to 0.00150\n",
      "Epoch: 37 / 100\n",
      "Epoch: [37][  1/707]\tTime  1.206 ( 1.206)\tData  1.055 ( 1.055)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 5.6925e-01 (5.6925e-01)\n",
      "Epoch: [37][178/707]\tTime  0.341 ( 0.345)\tData  0.213 ( 0.212)\tAcc@1 100.00 ( 92.35)\tAcc@5 100.00 (100.00)\tLoss 2.4297e-01 (2.3487e-01)\n",
      "Epoch: [37][355/707]\tTime  0.336 ( 0.342)\tData  0.209 ( 0.210)\tAcc@1  87.50 ( 90.92)\tAcc@5 100.00 (100.00)\tLoss 5.0684e-01 (2.5929e-01)\n",
      "Epoch: [37][532/707]\tTime  0.334 ( 0.341)\tData  0.208 ( 0.209)\tAcc@1  87.50 ( 90.81)\tAcc@5 100.00 (100.00)\tLoss 3.8423e-01 (2.6322e-01)\n",
      "Epoch: [37][707/707]\tTime  0.339 ( 0.340)\tData  0.215 ( 0.209)\tAcc@1  75.00 ( 90.84)\tAcc@5 100.00 (100.00)\tLoss 7.1707e-01 (2.6249e-01)\n",
      "[[ 339   71   22   27    7]\n",
      " [  48 1304   20   61   10]\n",
      " [  13   27  665   62    6]\n",
      " [  11   45   43 2545   14]\n",
      " [   4    6    4   17  285]]\n",
      "Epoch 37 perf acc@1: 90.84158325195312, perf acc@5: 100.0\n",
      "Best perf acc@1: 90.9830322265625, perf acc@5: 100.0 at epoch 36\n",
      "Epoch: 38 / 100\n",
      "Epoch: [38][  1/707]\tTime  1.218 ( 1.218)\tData  1.054 ( 1.054)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.5081e-01 (3.5081e-01)\n",
      "Epoch: [38][178/707]\tTime  0.345 ( 0.344)\tData  0.207 ( 0.212)\tAcc@1 100.00 ( 90.73)\tAcc@5 100.00 (100.00)\tLoss 1.1225e-02 (2.6788e-01)\n",
      "Epoch: [38][355/707]\tTime  0.331 ( 0.342)\tData  0.203 ( 0.210)\tAcc@1  87.50 ( 91.27)\tAcc@5 100.00 (100.00)\tLoss 3.9518e-01 (2.5431e-01)\n",
      "Epoch: [38][532/707]\tTime  0.336 ( 0.341)\tData  0.199 ( 0.209)\tAcc@1 100.00 ( 91.38)\tAcc@5 100.00 (100.00)\tLoss 1.7272e-01 (2.5317e-01)\n",
      "Epoch: [38][707/707]\tTime  0.336 ( 0.340)\tData  0.213 ( 0.209)\tAcc@1 100.00 ( 91.05)\tAcc@5 100.00 (100.00)\tLoss 1.3679e-02 (2.5340e-01)\n",
      "[[ 331   78   18   30    9]\n",
      " [  48 1310   23   54    8]\n",
      " [   7   24  658   79    5]\n",
      " [  10   40   38 2560   10]\n",
      " [   5    8    4    8  291]]\n",
      "Epoch 38 perf acc@1: 91.05374908447266, perf acc@5: 100.0\n",
      "Best perf acc@1: 91.05374908447266, perf acc@5: 100.0 at epoch 38\n",
      "Decreasing learning rate to 0.00135\n",
      "Epoch: 39 / 100\n",
      "Epoch: [39][  1/707]\tTime  1.000 ( 1.000)\tData  0.842 ( 0.842)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.5216e-01 (1.5216e-01)\n",
      "Epoch: [39][178/707]\tTime  0.337 ( 0.344)\tData  0.209 ( 0.210)\tAcc@1 100.00 ( 92.06)\tAcc@5 100.00 (100.00)\tLoss 9.3205e-02 (2.2324e-01)\n",
      "Epoch: [39][355/707]\tTime  0.352 ( 0.342)\tData  0.211 ( 0.208)\tAcc@1  75.00 ( 91.73)\tAcc@5 100.00 (100.00)\tLoss 9.3654e-01 (2.4996e-01)\n",
      "Epoch: [39][532/707]\tTime  0.340 ( 0.341)\tData  0.205 ( 0.208)\tAcc@1 100.00 ( 91.54)\tAcc@5 100.00 (100.00)\tLoss 4.4178e-02 (2.5271e-01)\n",
      "Epoch: [39][707/707]\tTime  0.337 ( 0.341)\tData  0.215 ( 0.208)\tAcc@1  87.50 ( 91.60)\tAcc@5 100.00 (100.00)\tLoss 5.4684e-01 (2.4878e-01)\n",
      "[[ 338   78   18   29    3]\n",
      " [  51 1302   23   57   10]\n",
      " [   6   15  681   65    6]\n",
      " [  10   43   35 2561    9]\n",
      " [   4    2    2    9  299]]\n",
      "Epoch 39 perf acc@1: 91.60183715820312, perf acc@5: 100.0\n",
      "Best perf acc@1: 91.60183715820312, perf acc@5: 100.0 at epoch 39\n",
      "Epoch: 40 / 100\n",
      "Epoch: [40][  1/707]\tTime  1.233 ( 1.233)\tData  1.086 ( 1.086)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.2952e-02 (4.2952e-02)\n",
      "Epoch: [40][178/707]\tTime  0.334 ( 0.346)\tData  0.207 ( 0.209)\tAcc@1 100.00 ( 92.28)\tAcc@5 100.00 (100.00)\tLoss 1.0354e-01 (2.3453e-01)\n",
      "Epoch: [40][355/707]\tTime  0.345 ( 0.343)\tData  0.206 ( 0.209)\tAcc@1 100.00 ( 92.15)\tAcc@5 100.00 (100.00)\tLoss 2.3294e-02 (2.2743e-01)\n",
      "Epoch: [40][532/707]\tTime  0.360 ( 0.342)\tData  0.207 ( 0.208)\tAcc@1 100.00 ( 91.35)\tAcc@5 100.00 (100.00)\tLoss 4.0875e-02 (2.4368e-01)\n",
      "Epoch: [40][707/707]\tTime  0.338 ( 0.341)\tData  0.213 ( 0.207)\tAcc@1 100.00 ( 91.50)\tAcc@5 100.00 (100.00)\tLoss 5.5224e-02 (2.4654e-01)\n",
      "[[ 332   77   20   32    5]\n",
      " [  51 1309   14   61    8]\n",
      " [   5   17  678   66    7]\n",
      " [   9   40   34 2566    9]\n",
      " [   4    9    3   10  290]]\n",
      "Epoch 40 perf acc@1: 91.49575805664062, perf acc@5: 100.0\n",
      "Best perf acc@1: 91.60183715820312, perf acc@5: 100.0 at epoch 39\n",
      "Decreasing learning rate to 0.00122\n",
      "Epoch: 41 / 100\n",
      "Epoch: [41][  1/707]\tTime  1.065 ( 1.065)\tData  0.914 ( 0.914)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.5966e-02 (4.5966e-02)\n",
      "Epoch: [41][178/707]\tTime  0.341 ( 0.344)\tData  0.207 ( 0.210)\tAcc@1 100.00 ( 91.22)\tAcc@5 100.00 (100.00)\tLoss 8.7986e-02 (2.2625e-01)\n",
      "Epoch: [41][355/707]\tTime  0.351 ( 0.343)\tData  0.210 ( 0.208)\tAcc@1 100.00 ( 90.77)\tAcc@5 100.00 (100.00)\tLoss 5.7402e-02 (2.4478e-01)\n",
      "Epoch: [41][532/707]\tTime  0.335 ( 0.343)\tData  0.203 ( 0.207)\tAcc@1 100.00 ( 91.42)\tAcc@5 100.00 (100.00)\tLoss 8.3193e-02 (2.3980e-01)\n",
      "Epoch: [41][707/707]\tTime  0.343 ( 0.342)\tData  0.218 ( 0.206)\tAcc@1  87.50 ( 91.62)\tAcc@5 100.00 (100.00)\tLoss 1.6962e-01 (2.4068e-01)\n",
      "[[ 350   70   14   22   10]\n",
      " [  44 1311   16   60   12]\n",
      " [   9   19  680   65    0]\n",
      " [   8   44   46 2549   11]\n",
      " [   2    9    3   10  292]]\n",
      "Epoch 41 perf acc@1: 91.61952209472656, perf acc@5: 100.0\n",
      "Best perf acc@1: 91.61952209472656, perf acc@5: 100.0 at epoch 41\n",
      "Epoch: 42 / 100\n",
      "Epoch: [42][  1/707]\tTime  1.004 ( 1.004)\tData  0.847 ( 0.847)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 2.4555e-02 (2.4555e-02)\n",
      "Epoch: [42][178/707]\tTime  0.340 ( 0.345)\tData  0.207 ( 0.210)\tAcc@1 100.00 ( 91.85)\tAcc@5 100.00 (100.00)\tLoss 1.1075e-01 (2.4866e-01)\n",
      "Epoch: [42][355/707]\tTime  0.337 ( 0.343)\tData  0.207 ( 0.207)\tAcc@1 100.00 ( 92.68)\tAcc@5 100.00 (100.00)\tLoss 7.3044e-02 (2.2995e-01)\n",
      "Epoch: [42][532/707]\tTime  0.341 ( 0.342)\tData  0.208 ( 0.207)\tAcc@1 100.00 ( 92.32)\tAcc@5 100.00 (100.00)\tLoss 8.3038e-02 (2.3634e-01)\n",
      "Epoch: [42][707/707]\tTime  0.333 ( 0.341)\tData  0.211 ( 0.206)\tAcc@1 100.00 ( 92.10)\tAcc@5 100.00 (100.00)\tLoss 4.5014e-02 (2.4050e-01)\n",
      "[[ 349   69   14   29    5]\n",
      " [  38 1322   19   56    8]\n",
      " [   9   23  674   61    6]\n",
      " [   9   36   30 2574    9]\n",
      " [   6    4    4   12  290]]\n",
      "Epoch 42 perf acc@1: 92.09689331054688, perf acc@5: 100.0\n",
      "Best perf acc@1: 92.09689331054688, perf acc@5: 100.0 at epoch 42\n",
      "Decreasing learning rate to 0.00109\n",
      "Epoch: 43 / 100\n",
      "Epoch: [43][  1/707]\tTime  1.305 ( 1.305)\tData  1.136 ( 1.136)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.4050e-01 (2.4050e-01)\n",
      "Epoch: [43][178/707]\tTime  0.337 ( 0.346)\tData  0.206 ( 0.211)\tAcc@1 100.00 ( 91.43)\tAcc@5 100.00 (100.00)\tLoss 5.5157e-02 (2.3062e-01)\n",
      "Epoch: [43][355/707]\tTime  0.340 ( 0.342)\tData  0.210 ( 0.208)\tAcc@1  87.50 ( 91.97)\tAcc@5 100.00 (100.00)\tLoss 3.7828e-01 (2.1594e-01)\n",
      "Epoch: [43][532/707]\tTime  0.337 ( 0.342)\tData  0.208 ( 0.208)\tAcc@1 100.00 ( 91.85)\tAcc@5 100.00 (100.00)\tLoss 7.7073e-02 (2.2239e-01)\n",
      "Epoch: [43][707/707]\tTime  0.332 ( 0.341)\tData  0.209 ( 0.207)\tAcc@1 100.00 ( 91.90)\tAcc@5 100.00 (100.00)\tLoss 9.7314e-02 (2.2692e-01)\n",
      "[[ 349   68   13   31    5]\n",
      " [  47 1311   14   61   10]\n",
      " [  13   17  673   63    7]\n",
      " [  14   31   35 2573    5]\n",
      " [   3    9    4    8  292]]\n",
      "Epoch 43 perf acc@1: 91.90240478515625, perf acc@5: 100.0\n",
      "Best perf acc@1: 92.09689331054688, perf acc@5: 100.0 at epoch 42\n",
      "Epoch: 44 / 100\n",
      "Epoch: [44][  1/707]\tTime  1.120 ( 1.120)\tData  0.955 ( 0.955)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.1587e-01 (2.1587e-01)\n",
      "Epoch: [44][178/707]\tTime  0.350 ( 0.345)\tData  0.211 ( 0.210)\tAcc@1 100.00 ( 92.84)\tAcc@5 100.00 (100.00)\tLoss 6.7538e-02 (2.1809e-01)\n",
      "Epoch: [44][355/707]\tTime  0.349 ( 0.343)\tData  0.208 ( 0.208)\tAcc@1 100.00 ( 92.99)\tAcc@5 100.00 (100.00)\tLoss 5.6394e-02 (2.1427e-01)\n",
      "Epoch: [44][532/707]\tTime  0.340 ( 0.342)\tData  0.209 ( 0.207)\tAcc@1  87.50 ( 93.30)\tAcc@5 100.00 (100.00)\tLoss 2.0461e-01 (2.0781e-01)\n",
      "Epoch: [44][707/707]\tTime  0.336 ( 0.341)\tData  0.213 ( 0.206)\tAcc@1 100.00 ( 93.05)\tAcc@5 100.00 (100.00)\tLoss 1.1120e-02 (2.1439e-01)\n",
      "[[ 352   64   13   26   11]\n",
      " [  46 1318   21   54    4]\n",
      " [  10   19  700   41    3]\n",
      " [   5   33   27 2587    6]\n",
      " [   1    7    1    1  306]]\n",
      "Epoch 44 perf acc@1: 93.05162811279297, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.05162811279297, perf acc@5: 100.0 at epoch 44\n",
      "Decreasing learning rate to 0.00098\n",
      "Epoch: 45 / 100\n",
      "Epoch: [45][  1/707]\tTime  1.058 ( 1.058)\tData  0.892 ( 0.892)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.2576e-01 (1.2576e-01)\n",
      "Epoch: [45][178/707]\tTime  0.324 ( 0.344)\tData  0.196 ( 0.209)\tAcc@1  75.00 ( 91.78)\tAcc@5 100.00 (100.00)\tLoss 5.4800e-01 (2.2829e-01)\n",
      "Epoch: [45][355/707]\tTime  0.336 ( 0.342)\tData  0.206 ( 0.207)\tAcc@1 100.00 ( 92.46)\tAcc@5 100.00 (100.00)\tLoss 9.9640e-02 (2.1145e-01)\n",
      "Epoch: [45][532/707]\tTime  0.348 ( 0.341)\tData  0.209 ( 0.207)\tAcc@1 100.00 ( 92.62)\tAcc@5 100.00 (100.00)\tLoss 4.3911e-02 (2.0734e-01)\n",
      "Epoch: [45][707/707]\tTime  0.341 ( 0.341)\tData  0.213 ( 0.206)\tAcc@1 100.00 ( 92.33)\tAcc@5 100.00 (100.00)\tLoss 4.2163e-02 (2.1727e-01)\n",
      "[[ 352   66   16   29    3]\n",
      " [  43 1320   26   50    4]\n",
      " [  10   21  679   60    3]\n",
      " [   8   34   30 2576   10]\n",
      " [   3    8    3    7  295]]\n",
      "Epoch 45 perf acc@1: 92.32673645019531, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.05162811279297, perf acc@5: 100.0 at epoch 44\n",
      "Epoch: 46 / 100\n",
      "Epoch: [46][  1/707]\tTime  1.198 ( 1.198)\tData  1.043 ( 1.043)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.1174e-01 (1.1174e-01)\n",
      "Epoch: [46][178/707]\tTime  0.330 ( 0.346)\tData  0.201 ( 0.208)\tAcc@1  87.50 ( 93.54)\tAcc@5 100.00 (100.00)\tLoss 5.8271e-01 (1.8823e-01)\n",
      "Epoch: [46][355/707]\tTime  0.351 ( 0.344)\tData  0.205 ( 0.207)\tAcc@1  87.50 ( 93.03)\tAcc@5 100.00 (100.00)\tLoss 2.5388e-01 (2.0349e-01)\n",
      "Epoch: [46][532/707]\tTime  0.340 ( 0.343)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 92.27)\tAcc@5 100.00 (100.00)\tLoss 1.4169e-01 (2.2309e-01)\n",
      "Epoch: [46][707/707]\tTime  0.340 ( 0.342)\tData  0.213 ( 0.206)\tAcc@1  87.50 ( 92.40)\tAcc@5 100.00 (100.00)\tLoss 7.3458e-01 (2.2318e-01)\n",
      "[[ 359   56   15   27    9]\n",
      " [  36 1323   19   58    7]\n",
      " [  12   18  685   55    3]\n",
      " [  11   44   36 2559    8]\n",
      " [   6    4    2    4  300]]\n",
      "Epoch 46 perf acc@1: 92.39745330810547, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.05162811279297, perf acc@5: 100.0 at epoch 44\n",
      "Decreasing learning rate to 0.00089\n",
      "Epoch: 47 / 100\n",
      "Epoch: [47][  1/707]\tTime  1.225 ( 1.225)\tData  1.068 ( 1.068)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.1242e-02 (4.1242e-02)\n",
      "Epoch: [47][178/707]\tTime  0.344 ( 0.346)\tData  0.208 ( 0.209)\tAcc@1 100.00 ( 93.40)\tAcc@5 100.00 (100.00)\tLoss 5.7776e-02 (2.0128e-01)\n",
      "Epoch: [47][355/707]\tTime  0.345 ( 0.343)\tData  0.211 ( 0.207)\tAcc@1  87.50 ( 93.70)\tAcc@5 100.00 (100.00)\tLoss 2.3830e-01 (1.9026e-01)\n",
      "Epoch: [47][532/707]\tTime  0.331 ( 0.342)\tData  0.198 ( 0.206)\tAcc@1  87.50 ( 93.56)\tAcc@5 100.00 (100.00)\tLoss 2.0044e-01 (1.9462e-01)\n",
      "Epoch: [47][707/707]\tTime  0.340 ( 0.341)\tData  0.213 ( 0.206)\tAcc@1  87.50 ( 93.39)\tAcc@5 100.00 (100.00)\tLoss 1.0076e+00 (2.0046e-01)\n",
      "[[ 376   51   12   23    4]\n",
      " [  33 1334   16   52    8]\n",
      " [   7   21  690   51    4]\n",
      " [   5   32   29 2589    3]\n",
      " [   4    7    2   10  293]]\n",
      "Epoch 47 perf acc@1: 93.38755798339844, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.38755798339844, perf acc@5: 100.0 at epoch 47\n",
      "Epoch: 48 / 100\n",
      "Epoch: [48][  1/707]\tTime  1.181 ( 1.181)\tData  1.005 ( 1.005)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 3.8279e-02 (3.8279e-02)\n",
      "Epoch: [48][178/707]\tTime  0.352 ( 0.346)\tData  0.204 ( 0.209)\tAcc@1 100.00 ( 93.68)\tAcc@5 100.00 (100.00)\tLoss 1.6510e-01 (1.8773e-01)\n",
      "Epoch: [48][355/707]\tTime  0.334 ( 0.343)\tData  0.203 ( 0.207)\tAcc@1 100.00 ( 93.45)\tAcc@5 100.00 (100.00)\tLoss 6.5282e-02 (1.9085e-01)\n",
      "Epoch: [48][532/707]\tTime  0.336 ( 0.342)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 93.19)\tAcc@5 100.00 (100.00)\tLoss 1.3891e-01 (1.9902e-01)\n",
      "Epoch: [48][707/707]\tTime  0.341 ( 0.342)\tData  0.216 ( 0.206)\tAcc@1  62.50 ( 93.14)\tAcc@5 100.00 (100.00)\tLoss 6.2330e-01 (2.0021e-01)\n",
      "[[ 362   59   17   25    3]\n",
      " [  36 1335   18   51    3]\n",
      " [   8   17  690   54    4]\n",
      " [   9   29   34 2580    6]\n",
      " [   1    6    2    6  301]]\n",
      "Epoch 48 perf acc@1: 93.14002990722656, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.38755798339844, perf acc@5: 100.0 at epoch 47\n",
      "Decreasing learning rate to 0.00080\n",
      "Epoch: 49 / 100\n",
      "Epoch: [49][  1/707]\tTime  1.297 ( 1.297)\tData  1.132 ( 1.132)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.1588e-02 (4.1588e-02)\n",
      "Epoch: [49][178/707]\tTime  0.339 ( 0.346)\tData  0.211 ( 0.208)\tAcc@1  87.50 ( 93.75)\tAcc@5 100.00 (100.00)\tLoss 1.7544e-01 (1.7623e-01)\n",
      "Epoch: [49][355/707]\tTime  0.343 ( 0.343)\tData  0.206 ( 0.207)\tAcc@1 100.00 ( 93.17)\tAcc@5 100.00 (100.00)\tLoss 1.1044e-01 (1.9499e-01)\n",
      "Epoch: [49][532/707]\tTime  0.322 ( 0.343)\tData  0.183 ( 0.206)\tAcc@1 100.00 ( 93.30)\tAcc@5 100.00 (100.00)\tLoss 4.3641e-03 (1.9306e-01)\n",
      "Epoch: [49][707/707]\tTime  0.339 ( 0.342)\tData  0.214 ( 0.205)\tAcc@1  87.50 ( 93.26)\tAcc@5 100.00 (100.00)\tLoss 1.5635e-01 (1.9382e-01)\n",
      "[[ 362   57   20   23    4]\n",
      " [  33 1348   14   42    6]\n",
      " [   9   17  686   59    2]\n",
      " [  12   25   32 2580    9]\n",
      " [   3    3    3    8  299]]\n",
      "Epoch 49 perf acc@1: 93.2637939453125, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.38755798339844, perf acc@5: 100.0 at epoch 47\n",
      "Epoch: 50 / 100\n",
      "Epoch: [50][  1/707]\tTime  1.317 ( 1.317)\tData  1.172 ( 1.172)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.3538e-01 (2.3538e-01)\n",
      "Epoch: [50][178/707]\tTime  0.341 ( 0.346)\tData  0.209 ( 0.209)\tAcc@1 100.00 ( 92.91)\tAcc@5 100.00 (100.00)\tLoss 2.9228e-03 (1.8394e-01)\n",
      "Epoch: [50][355/707]\tTime  0.357 ( 0.344)\tData  0.226 ( 0.206)\tAcc@1 100.00 ( 93.49)\tAcc@5 100.00 (100.00)\tLoss 2.3150e-02 (1.8980e-01)\n",
      "Epoch: [50][532/707]\tTime  0.321 ( 0.343)\tData  0.194 ( 0.205)\tAcc@1 100.00 ( 93.59)\tAcc@5 100.00 (100.00)\tLoss 3.0643e-02 (1.8564e-01)\n",
      "Epoch: [50][707/707]\tTime  0.334 ( 0.342)\tData  0.210 ( 0.205)\tAcc@1  87.50 ( 93.51)\tAcc@5 100.00 (100.00)\tLoss 3.0029e-01 (1.8480e-01)\n",
      "[[ 372   53   16   23    2]\n",
      " [  38 1335   18   45    7]\n",
      " [   7   20  696   49    1]\n",
      " [   5   29   30 2586    8]\n",
      " [   4    4    2    6  300]]\n",
      "Epoch 50 perf acc@1: 93.51131439208984, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.51131439208984, perf acc@5: 100.0 at epoch 50\n",
      "Decreasing learning rate to 0.00072\n",
      "Epoch: 51 / 100\n",
      "Epoch: [51][  1/707]\tTime  1.412 ( 1.412)\tData  1.256 ( 1.256)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 1.1564e-01 (1.1564e-01)\n",
      "Epoch: [51][178/707]\tTime  0.339 ( 0.347)\tData  0.204 ( 0.208)\tAcc@1 100.00 ( 93.40)\tAcc@5 100.00 (100.00)\tLoss 1.3764e-02 (1.8120e-01)\n",
      "Epoch: [51][355/707]\tTime  0.359 ( 0.344)\tData  0.207 ( 0.205)\tAcc@1 100.00 ( 93.59)\tAcc@5 100.00 (100.00)\tLoss 1.7643e-02 (1.8535e-01)\n",
      "Epoch: [51][532/707]\tTime  0.328 ( 0.343)\tData  0.191 ( 0.204)\tAcc@1 100.00 ( 93.66)\tAcc@5 100.00 (100.00)\tLoss 4.5477e-02 (1.8856e-01)\n",
      "Epoch: [51][707/707]\tTime  0.332 ( 0.342)\tData  0.208 ( 0.204)\tAcc@1 100.00 ( 93.30)\tAcc@5 100.00 (100.00)\tLoss 1.0479e-01 (1.9213e-01)\n",
      "[[ 358   56   15   29    8]\n",
      " [  45 1337   21   38    2]\n",
      " [  11   21  685   54    2]\n",
      " [   9   23   27 2594    5]\n",
      " [   1    5    2    5  303]]\n",
      "Epoch 51 perf acc@1: 93.29915618896484, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.51131439208984, perf acc@5: 100.0 at epoch 50\n",
      "Epoch: 52 / 100\n",
      "Epoch: [52][  1/707]\tTime  1.175 ( 1.175)\tData  1.018 ( 1.018)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.5038e-01 (1.5038e-01)\n",
      "Epoch: [52][178/707]\tTime  0.343 ( 0.347)\tData  0.201 ( 0.207)\tAcc@1 100.00 ( 94.03)\tAcc@5 100.00 (100.00)\tLoss 7.1435e-03 (1.6688e-01)\n",
      "Epoch: [52][355/707]\tTime  0.354 ( 0.344)\tData  0.212 ( 0.205)\tAcc@1 100.00 ( 93.73)\tAcc@5 100.00 (100.00)\tLoss 7.9222e-02 (1.8247e-01)\n",
      "Epoch: [52][532/707]\tTime  0.325 ( 0.343)\tData  0.196 ( 0.204)\tAcc@1 100.00 ( 93.68)\tAcc@5 100.00 (100.00)\tLoss 4.4128e-03 (1.8882e-01)\n",
      "Epoch: [52][707/707]\tTime  0.338 ( 0.343)\tData  0.209 ( 0.204)\tAcc@1 100.00 ( 93.65)\tAcc@5 100.00 (100.00)\tLoss 5.0433e-03 (1.9029e-01)\n",
      "[[ 374   58   14   16    4]\n",
      " [  40 1345   16   36    6]\n",
      " [   8   18  692   51    4]\n",
      " [   9   36   28 2582    3]\n",
      " [   3    4    3    2  304]]\n",
      "Epoch 52 perf acc@1: 93.65276336669922, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.65276336669922, perf acc@5: 100.0 at epoch 52\n",
      "Decreasing learning rate to 0.00065\n",
      "Epoch: 53 / 100\n",
      "Epoch: [53][  1/707]\tTime  1.369 ( 1.369)\tData  1.204 ( 1.204)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.2624e-01 (3.2624e-01)\n",
      "Epoch: [53][178/707]\tTime  0.339 ( 0.347)\tData  0.209 ( 0.208)\tAcc@1  87.50 ( 94.10)\tAcc@5 100.00 (100.00)\tLoss 4.7783e-01 (1.8186e-01)\n",
      "Epoch: [53][355/707]\tTime  0.339 ( 0.343)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 93.70)\tAcc@5 100.00 (100.00)\tLoss 1.0305e-01 (1.8105e-01)\n",
      "Epoch: [53][532/707]\tTime  0.339 ( 0.342)\tData  0.202 ( 0.205)\tAcc@1 100.00 ( 93.70)\tAcc@5 100.00 (100.00)\tLoss 5.8541e-02 (1.8000e-01)\n",
      "Epoch: [53][707/707]\tTime  0.344 ( 0.342)\tData  0.211 ( 0.205)\tAcc@1  75.00 ( 93.64)\tAcc@5 100.00 (100.00)\tLoss 1.2184e+00 (1.8104e-01)\n",
      "[[ 373   50   16   23    4]\n",
      " [  35 1348   13   42    5]\n",
      " [   8   17  692   54    2]\n",
      " [   8   32   31 2580    7]\n",
      " [   1    2    1    9  303]]\n",
      "Epoch 53 perf acc@1: 93.63507843017578, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.65276336669922, perf acc@5: 100.0 at epoch 52\n",
      "Epoch: 54 / 100\n",
      "Epoch: [54][  1/707]\tTime  1.262 ( 1.262)\tData  1.109 ( 1.109)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 1.3142e-01 (1.3142e-01)\n",
      "Epoch: [54][178/707]\tTime  0.362 ( 0.346)\tData  0.209 ( 0.208)\tAcc@1 100.00 ( 94.38)\tAcc@5 100.00 (100.00)\tLoss 8.5688e-02 (1.5575e-01)\n",
      "Epoch: [54][355/707]\tTime  0.334 ( 0.343)\tData  0.198 ( 0.206)\tAcc@1 100.00 ( 93.66)\tAcc@5 100.00 (100.00)\tLoss 3.0707e-02 (1.8027e-01)\n",
      "Epoch: [54][532/707]\tTime  0.340 ( 0.343)\tData  0.208 ( 0.205)\tAcc@1 100.00 ( 93.44)\tAcc@5 100.00 (100.00)\tLoss 7.1243e-02 (1.8343e-01)\n",
      "Epoch: [54][707/707]\tTime  0.334 ( 0.342)\tData  0.210 ( 0.205)\tAcc@1  87.50 ( 93.58)\tAcc@5 100.00 (100.00)\tLoss 2.1163e-01 (1.8034e-01)\n",
      "[[ 370   53   17   22    4]\n",
      " [  38 1347   15   35    8]\n",
      " [  10   10  697   55    1]\n",
      " [  11   28   37 2573    9]\n",
      " [   2    2    2    4  306]]\n",
      "Epoch 54 perf acc@1: 93.58203887939453, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.65276336669922, perf acc@5: 100.0 at epoch 52\n",
      "Decreasing learning rate to 0.00058\n",
      "Epoch: 55 / 100\n",
      "Epoch: [55][  1/707]\tTime  1.283 ( 1.283)\tData  1.125 ( 1.125)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 3.5630e-02 (3.5630e-02)\n",
      "Epoch: [55][178/707]\tTime  0.333 ( 0.347)\tData  0.203 ( 0.209)\tAcc@1  87.50 ( 93.47)\tAcc@5 100.00 (100.00)\tLoss 1.2432e-01 (1.8130e-01)\n",
      "Epoch: [55][355/707]\tTime  0.347 ( 0.343)\tData  0.211 ( 0.206)\tAcc@1 100.00 ( 93.59)\tAcc@5 100.00 (100.00)\tLoss 1.0519e-01 (1.7367e-01)\n",
      "Epoch: [55][532/707]\tTime  0.341 ( 0.343)\tData  0.203 ( 0.205)\tAcc@1  87.50 ( 93.91)\tAcc@5 100.00 (100.00)\tLoss 1.7404e-01 (1.6625e-01)\n",
      "Epoch: [55][707/707]\tTime  0.329 ( 0.342)\tData  0.205 ( 0.205)\tAcc@1 100.00 ( 93.92)\tAcc@5 100.00 (100.00)\tLoss 5.3717e-02 (1.6550e-01)\n",
      "[[ 377   46   20   21    2]\n",
      " [  40 1334   18   48    3]\n",
      " [  10   18  699   45    1]\n",
      " [   9   26   26 2592    5]\n",
      " [   2    2    0    2  310]]\n",
      "Epoch 55 perf acc@1: 93.91796875, perf acc@5: 100.0\n",
      "Best perf acc@1: 93.91796875, perf acc@5: 100.0 at epoch 55\n",
      "Epoch: 56 / 100\n",
      "Epoch: [56][  1/707]\tTime  1.148 ( 1.148)\tData  0.995 ( 0.995)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.1890e-02 (4.1890e-02)\n",
      "Epoch: [56][178/707]\tTime  0.344 ( 0.346)\tData  0.207 ( 0.209)\tAcc@1  87.50 ( 94.31)\tAcc@5 100.00 (100.00)\tLoss 3.4007e-01 (1.6299e-01)\n",
      "Epoch: [56][355/707]\tTime  0.375 ( 0.343)\tData  0.202 ( 0.206)\tAcc@1 100.00 ( 94.65)\tAcc@5 100.00 (100.00)\tLoss 6.3880e-02 (1.5952e-01)\n",
      "Epoch: [56][532/707]\tTime  0.332 ( 0.343)\tData  0.198 ( 0.205)\tAcc@1 100.00 ( 94.27)\tAcc@5 100.00 (100.00)\tLoss 4.1314e-02 (1.6811e-01)\n",
      "Epoch: [56][707/707]\tTime  0.339 ( 0.342)\tData  0.210 ( 0.205)\tAcc@1  87.50 ( 94.20)\tAcc@5 100.00 (100.00)\tLoss 3.5595e-01 (1.7094e-01)\n",
      "[[ 384   46   11   23    2]\n",
      " [  28 1353   15   44    3]\n",
      " [  10   16  697   46    4]\n",
      " [   7   25   25 2592    9]\n",
      " [   1    3    2    8  302]]\n",
      "Epoch 56 perf acc@1: 94.20085144042969, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.20085144042969, perf acc@5: 100.0 at epoch 56\n",
      "Decreasing learning rate to 0.00052\n",
      "Epoch: 57 / 100\n",
      "Epoch: [57][  1/707]\tTime  1.156 ( 1.156)\tData  0.973 ( 0.973)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.3843e-01 (3.3843e-01)\n",
      "Epoch: [57][178/707]\tTime  0.339 ( 0.345)\tData  0.204 ( 0.209)\tAcc@1 100.00 ( 94.52)\tAcc@5 100.00 (100.00)\tLoss 1.4045e-01 (1.6206e-01)\n",
      "Epoch: [57][355/707]\tTime  0.341 ( 0.343)\tData  0.211 ( 0.207)\tAcc@1  87.50 ( 93.66)\tAcc@5 100.00 (100.00)\tLoss 2.9023e-01 (1.8654e-01)\n",
      "Epoch: [57][532/707]\tTime  0.336 ( 0.341)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 93.98)\tAcc@5 100.00 (100.00)\tLoss 6.8226e-02 (1.8121e-01)\n",
      "Epoch: [57][707/707]\tTime  0.338 ( 0.341)\tData  0.215 ( 0.206)\tAcc@1 100.00 ( 93.95)\tAcc@5 100.00 (100.00)\tLoss 1.9959e-02 (1.7637e-01)\n",
      "[[ 384   41   14   21    6]\n",
      " [  27 1348   22   43    3]\n",
      " [   7   16  690   57    3]\n",
      " [   9   29   26 2590    4]\n",
      " [   4    3    1    6  302]]\n",
      "Epoch 57 perf acc@1: 93.95332336425781, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.20085144042969, perf acc@5: 100.0 at epoch 56\n",
      "Epoch: 58 / 100\n",
      "Epoch: [58][  1/707]\tTime  1.109 ( 1.109)\tData  0.920 ( 0.920)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 7.1631e-02 (7.1631e-02)\n",
      "Epoch: [58][178/707]\tTime  0.342 ( 0.345)\tData  0.204 ( 0.209)\tAcc@1 100.00 ( 95.58)\tAcc@5 100.00 (100.00)\tLoss 9.9835e-03 (1.5568e-01)\n",
      "Epoch: [58][355/707]\tTime  0.332 ( 0.342)\tData  0.197 ( 0.207)\tAcc@1 100.00 ( 94.89)\tAcc@5 100.00 (100.00)\tLoss 3.8710e-02 (1.6391e-01)\n",
      "Epoch: [58][532/707]\tTime  0.336 ( 0.341)\tData  0.205 ( 0.207)\tAcc@1 100.00 ( 94.62)\tAcc@5 100.00 (100.00)\tLoss 1.1533e-01 (1.6400e-01)\n",
      "Epoch: [58][707/707]\tTime  0.339 ( 0.341)\tData  0.213 ( 0.207)\tAcc@1  87.50 ( 94.55)\tAcc@5 100.00 (100.00)\tLoss 8.5747e-01 (1.6715e-01)\n",
      "[[ 382   47   11   19    7]\n",
      " [  33 1347   14   46    3]\n",
      " [   6   10  718   35    4]\n",
      " [   9   25   20 2597    7]\n",
      " [   3    3    1    5  304]]\n",
      "Epoch 58 perf acc@1: 94.55445861816406, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.55445861816406, perf acc@5: 100.0 at epoch 58\n",
      "Decreasing learning rate to 0.00047\n",
      "Epoch: 59 / 100\n",
      "Epoch: [59][  1/707]\tTime  1.393 ( 1.393)\tData  1.228 ( 1.228)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 2.8627e-02 (2.8627e-02)\n",
      "Epoch: [59][178/707]\tTime  0.327 ( 0.347)\tData  0.199 ( 0.212)\tAcc@1 100.00 ( 95.08)\tAcc@5 100.00 (100.00)\tLoss 6.0377e-03 (1.4776e-01)\n",
      "Epoch: [59][355/707]\tTime  0.345 ( 0.343)\tData  0.212 ( 0.208)\tAcc@1 100.00 ( 94.89)\tAcc@5 100.00 (100.00)\tLoss 3.7927e-02 (1.4955e-01)\n",
      "Epoch: [59][532/707]\tTime  0.327 ( 0.342)\tData  0.198 ( 0.208)\tAcc@1  75.00 ( 94.78)\tAcc@5 100.00 (100.00)\tLoss 1.1314e+00 (1.5508e-01)\n",
      "Epoch: [59][707/707]\tTime  0.335 ( 0.341)\tData  0.211 ( 0.207)\tAcc@1  87.50 ( 94.41)\tAcc@5 100.00 (100.00)\tLoss 2.4894e-01 (1.6570e-01)\n",
      "[[ 387   43   17   16    3]\n",
      " [  27 1365   10   39    2]\n",
      " [   6   13  703   46    5]\n",
      " [   7   28   31 2584    8]\n",
      " [   2    3    3    7  301]]\n",
      "Epoch 59 perf acc@1: 94.41301727294922, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.55445861816406, perf acc@5: 100.0 at epoch 58\n",
      "Epoch: 60 / 100\n",
      "Epoch: [60][  1/707]\tTime  1.295 ( 1.295)\tData  1.137 ( 1.137)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.8439e-01 (3.8439e-01)\n",
      "Epoch: [60][178/707]\tTime  0.338 ( 0.346)\tData  0.208 ( 0.212)\tAcc@1 100.00 ( 95.22)\tAcc@5 100.00 (100.00)\tLoss 2.9080e-02 (1.5655e-01)\n",
      "Epoch: [60][355/707]\tTime  0.347 ( 0.343)\tData  0.212 ( 0.209)\tAcc@1 100.00 ( 95.00)\tAcc@5 100.00 (100.00)\tLoss 2.2520e-02 (1.5384e-01)\n",
      "Epoch: [60][532/707]\tTime  0.336 ( 0.342)\tData  0.206 ( 0.208)\tAcc@1 100.00 ( 94.78)\tAcc@5 100.00 (100.00)\tLoss 2.1478e-03 (1.5682e-01)\n",
      "Epoch: [60][707/707]\tTime  0.333 ( 0.341)\tData  0.210 ( 0.208)\tAcc@1 100.00 ( 94.96)\tAcc@5 100.00 (100.00)\tLoss 6.1977e-02 (1.5163e-01)\n",
      "[[ 396   36   16   17    1]\n",
      " [  33 1361   12   37    0]\n",
      " [   7   10  712   41    3]\n",
      " [   6   22   29 2596    5]\n",
      " [   3    1    1    5  306]]\n",
      "Epoch 60 perf acc@1: 94.96110534667969, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Decreasing learning rate to 0.00042\n",
      "Epoch: 61 / 100\n",
      "Epoch: [61][  1/707]\tTime  1.108 ( 1.108)\tData  0.954 ( 0.954)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 5.1505e-01 (5.1505e-01)\n",
      "Epoch: [61][178/707]\tTime  0.358 ( 0.346)\tData  0.212 ( 0.209)\tAcc@1  87.50 ( 93.54)\tAcc@5 100.00 (100.00)\tLoss 2.4547e-01 (1.7930e-01)\n",
      "Epoch: [61][355/707]\tTime  0.341 ( 0.343)\tData  0.206 ( 0.206)\tAcc@1 100.00 ( 93.73)\tAcc@5 100.00 (100.00)\tLoss 7.0541e-02 (1.7440e-01)\n",
      "Epoch: [61][532/707]\tTime  0.343 ( 0.342)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 94.36)\tAcc@5 100.00 (100.00)\tLoss 6.4747e-02 (1.5898e-01)\n",
      "Epoch: [61][707/707]\tTime  0.334 ( 0.341)\tData  0.211 ( 0.206)\tAcc@1 100.00 ( 94.36)\tAcc@5 100.00 (100.00)\tLoss 1.9746e-02 (1.6042e-01)\n",
      "[[ 390   39   14   21    2]\n",
      " [  34 1351   14   43    1]\n",
      " [   7   16  706   41    3]\n",
      " [   9   35   20 2586    8]\n",
      " [   0    4    2    6  304]]\n",
      "Epoch 61 perf acc@1: 94.35997009277344, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Epoch: 62 / 100\n",
      "Epoch: [62][  1/707]\tTime  1.124 ( 1.124)\tData  0.975 ( 0.975)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 3.4530e-01 (3.4530e-01)\n",
      "Epoch: [62][178/707]\tTime  0.362 ( 0.344)\tData  0.208 ( 0.210)\tAcc@1 100.00 ( 94.17)\tAcc@5 100.00 (100.00)\tLoss 4.7436e-02 (1.6580e-01)\n",
      "Epoch: [62][355/707]\tTime  0.350 ( 0.343)\tData  0.197 ( 0.207)\tAcc@1  87.50 ( 94.40)\tAcc@5 100.00 (100.00)\tLoss 2.2069e-01 (1.5998e-01)\n",
      "Epoch: [62][532/707]\tTime  0.329 ( 0.342)\tData  0.192 ( 0.206)\tAcc@1 100.00 ( 94.36)\tAcc@5 100.00 (100.00)\tLoss 9.0732e-02 (1.6239e-01)\n",
      "Epoch: [62][707/707]\tTime  0.340 ( 0.342)\tData  0.216 ( 0.205)\tAcc@1  87.50 ( 94.31)\tAcc@5 100.00 (100.00)\tLoss 2.6827e-01 (1.6219e-01)\n",
      "[[ 381   48   17   16    4]\n",
      " [  32 1355   18   36    2]\n",
      " [   8   13  704   46    2]\n",
      " [   7   27   25 2593    6]\n",
      " [   1    4    3    7  301]]\n",
      "Epoch 62 perf acc@1: 94.30693054199219, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Decreasing learning rate to 0.00038\n",
      "Epoch: 63 / 100\n",
      "Epoch: [63][  1/707]\tTime  1.220 ( 1.220)\tData  1.061 ( 1.061)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 5.4388e-02 (5.4388e-02)\n",
      "Epoch: [63][178/707]\tTime  0.336 ( 0.346)\tData  0.204 ( 0.209)\tAcc@1  87.50 ( 94.31)\tAcc@5 100.00 (100.00)\tLoss 3.7843e-01 (1.7854e-01)\n",
      "Epoch: [63][355/707]\tTime  0.346 ( 0.343)\tData  0.209 ( 0.208)\tAcc@1 100.00 ( 94.40)\tAcc@5 100.00 (100.00)\tLoss 2.2813e-02 (1.6981e-01)\n",
      "Epoch: [63][532/707]\tTime  0.333 ( 0.343)\tData  0.202 ( 0.207)\tAcc@1  87.50 ( 94.53)\tAcc@5 100.00 (100.00)\tLoss 2.6426e-01 (1.7015e-01)\n",
      "Epoch: [63][707/707]\tTime  0.336 ( 0.342)\tData  0.211 ( 0.206)\tAcc@1 100.00 ( 94.47)\tAcc@5 100.00 (100.00)\tLoss 1.6980e-02 (1.6819e-01)\n",
      "[[ 389   46    9   17    5]\n",
      " [  33 1356   11   39    4]\n",
      " [   8   15  695   52    3]\n",
      " [   2   34   25 2594    3]\n",
      " [   2    1    0    4  309]]\n",
      "Epoch 63 perf acc@1: 94.46605682373047, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Epoch: 64 / 100\n",
      "Epoch: [64][  1/707]\tTime  1.267 ( 1.267)\tData  1.116 ( 1.116)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.4700e-02 (4.4700e-02)\n",
      "Epoch: [64][178/707]\tTime  0.358 ( 0.346)\tData  0.214 ( 0.210)\tAcc@1 100.00 ( 95.29)\tAcc@5 100.00 (100.00)\tLoss 1.5683e-02 (1.3944e-01)\n",
      "Epoch: [64][355/707]\tTime  0.332 ( 0.343)\tData  0.204 ( 0.207)\tAcc@1 100.00 ( 94.79)\tAcc@5 100.00 (100.00)\tLoss 3.3520e-02 (1.5735e-01)\n",
      "Epoch: [64][532/707]\tTime  0.343 ( 0.342)\tData  0.208 ( 0.206)\tAcc@1  87.50 ( 94.53)\tAcc@5 100.00 (100.00)\tLoss 2.2854e-01 (1.6156e-01)\n",
      "Epoch: [64][707/707]\tTime  0.334 ( 0.342)\tData  0.210 ( 0.206)\tAcc@1  87.50 ( 94.68)\tAcc@5 100.00 (100.00)\tLoss 2.1757e-01 (1.6123e-01)\n",
      "[[ 388   44   13   19    2]\n",
      " [  29 1365   10   36    3]\n",
      " [   3   18  707   44    1]\n",
      " [   5   25   30 2594    4]\n",
      " [   2    4    6    3  301]]\n",
      "Epoch 64 perf acc@1: 94.67822265625, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Decreasing learning rate to 0.00034\n",
      "Epoch: 65 / 100\n",
      "Epoch: [65][  1/707]\tTime  1.159 ( 1.159)\tData  1.008 ( 1.008)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.3843e-01 (2.3843e-01)\n",
      "Epoch: [65][178/707]\tTime  0.350 ( 0.345)\tData  0.210 ( 0.209)\tAcc@1 100.00 ( 94.80)\tAcc@5 100.00 (100.00)\tLoss 7.6863e-02 (1.3950e-01)\n",
      "Epoch: [65][355/707]\tTime  0.335 ( 0.343)\tData  0.200 ( 0.207)\tAcc@1 100.00 ( 94.68)\tAcc@5 100.00 (100.00)\tLoss 4.1117e-02 (1.4588e-01)\n",
      "Epoch: [65][532/707]\tTime  0.343 ( 0.342)\tData  0.209 ( 0.206)\tAcc@1 100.00 ( 94.64)\tAcc@5 100.00 (100.00)\tLoss 8.8453e-02 (1.4990e-01)\n",
      "Epoch: [65][707/707]\tTime  0.333 ( 0.341)\tData  0.210 ( 0.206)\tAcc@1 100.00 ( 94.59)\tAcc@5 100.00 (100.00)\tLoss 6.1847e-02 (1.4884e-01)\n",
      "[[ 393   40   13   14    6]\n",
      " [  26 1353   15   46    3]\n",
      " [   8   15  702   47    1]\n",
      " [   5   28   22 2599    4]\n",
      " [   1    3    3    6  303]]\n",
      "Epoch 65 perf acc@1: 94.5898208618164, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Epoch: 66 / 100\n",
      "Epoch: [66][  1/707]\tTime  1.322 ( 1.322)\tData  1.156 ( 1.156)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 2.6886e-02 (2.6886e-02)\n",
      "Epoch: [66][178/707]\tTime  0.334 ( 0.345)\tData  0.201 ( 0.209)\tAcc@1 100.00 ( 95.29)\tAcc@5 100.00 (100.00)\tLoss 6.2134e-02 (1.3366e-01)\n",
      "Epoch: [66][355/707]\tTime  0.342 ( 0.343)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 95.04)\tAcc@5 100.00 (100.00)\tLoss 6.5254e-03 (1.3836e-01)\n",
      "Epoch: [66][532/707]\tTime  0.355 ( 0.342)\tData  0.206 ( 0.206)\tAcc@1  87.50 ( 94.81)\tAcc@5 100.00 (100.00)\tLoss 1.3274e-01 (1.4403e-01)\n",
      "Epoch: [66][707/707]\tTime  0.333 ( 0.341)\tData  0.210 ( 0.206)\tAcc@1 100.00 ( 94.66)\tAcc@5 100.00 (100.00)\tLoss 3.9597e-02 (1.4903e-01)\n",
      "[[ 384   47    9   21    5]\n",
      " [  27 1357   11   42    6]\n",
      " [   5   14  720   33    1]\n",
      " [  11   29   25 2590    3]\n",
      " [   1    3    4    5  303]]\n",
      "Epoch 66 perf acc@1: 94.66053771972656, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Decreasing learning rate to 0.00031\n",
      "Epoch: 67 / 100\n",
      "Epoch: [67][  1/707]\tTime  1.361 ( 1.361)\tData  1.219 ( 1.219)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 1.8573e-01 (1.8573e-01)\n",
      "Epoch: [67][178/707]\tTime  0.350 ( 0.346)\tData  0.217 ( 0.211)\tAcc@1 100.00 ( 94.59)\tAcc@5 100.00 (100.00)\tLoss 1.7850e-01 (1.4155e-01)\n",
      "Epoch: [67][355/707]\tTime  0.346 ( 0.343)\tData  0.208 ( 0.209)\tAcc@1 100.00 ( 94.75)\tAcc@5 100.00 (100.00)\tLoss 1.2506e-02 (1.4521e-01)\n",
      "Epoch: [67][532/707]\tTime  0.343 ( 0.342)\tData  0.205 ( 0.207)\tAcc@1 100.00 ( 94.90)\tAcc@5 100.00 (100.00)\tLoss 6.8423e-02 (1.4245e-01)\n",
      "Epoch: [67][707/707]\tTime  0.334 ( 0.341)\tData  0.211 ( 0.207)\tAcc@1  75.00 ( 94.87)\tAcc@5 100.00 (100.00)\tLoss 5.8030e-01 (1.4389e-01)\n",
      "[[ 395   38   12   19    2]\n",
      " [  33 1352   14   42    2]\n",
      " [   4   12  716   39    2]\n",
      " [   4   26   25 2597    6]\n",
      " [   1    5    3    1  306]]\n",
      "Epoch 67 perf acc@1: 94.8727035522461, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Epoch: 68 / 100\n",
      "Epoch: [68][  1/707]\tTime  1.249 ( 1.249)\tData  1.099 ( 1.099)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.2524e-01 (1.2524e-01)\n",
      "Epoch: [68][178/707]\tTime  0.365 ( 0.345)\tData  0.217 ( 0.211)\tAcc@1 100.00 ( 94.45)\tAcc@5 100.00 (100.00)\tLoss 1.4949e-02 (1.5599e-01)\n",
      "Epoch: [68][355/707]\tTime  0.337 ( 0.342)\tData  0.205 ( 0.208)\tAcc@1 100.00 ( 94.93)\tAcc@5 100.00 (100.00)\tLoss 5.6314e-02 (1.4268e-01)\n",
      "Epoch: [68][532/707]\tTime  0.337 ( 0.341)\tData  0.207 ( 0.208)\tAcc@1  87.50 ( 94.69)\tAcc@5 100.00 (100.00)\tLoss 4.7047e-01 (1.4916e-01)\n",
      "Epoch: [68][707/707]\tTime  0.336 ( 0.340)\tData  0.214 ( 0.207)\tAcc@1 100.00 ( 94.80)\tAcc@5 100.00 (100.00)\tLoss 2.7499e-02 (1.4246e-01)\n",
      "[[ 398   37    9   17    5]\n",
      " [  25 1366   14   37    1]\n",
      " [   5   23  695   48    2]\n",
      " [  13   22   22 2599    2]\n",
      " [   2    5    2    3  304]]\n",
      "Epoch 68 perf acc@1: 94.8019790649414, perf acc@5: 100.0\n",
      "Best perf acc@1: 94.96110534667969, perf acc@5: 100.0 at epoch 60\n",
      "Decreasing learning rate to 0.00028\n",
      "Epoch: 69 / 100\n",
      "Epoch: [69][  1/707]\tTime  1.145 ( 1.145)\tData  0.971 ( 0.971)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.4808e-02 (1.4808e-02)\n",
      "Epoch: [69][178/707]\tTime  0.354 ( 0.345)\tData  0.215 ( 0.210)\tAcc@1  75.00 ( 95.51)\tAcc@5 100.00 (100.00)\tLoss 4.8584e-01 (1.3424e-01)\n",
      "Epoch: [69][355/707]\tTime  0.329 ( 0.342)\tData  0.197 ( 0.207)\tAcc@1 100.00 ( 95.32)\tAcc@5 100.00 (100.00)\tLoss 1.7527e-02 (1.3516e-01)\n",
      "Epoch: [69][532/707]\tTime  0.340 ( 0.342)\tData  0.211 ( 0.206)\tAcc@1 100.00 ( 95.39)\tAcc@5 100.00 (100.00)\tLoss 9.8477e-02 (1.3497e-01)\n",
      "Epoch: [69][707/707]\tTime  0.338 ( 0.341)\tData  0.214 ( 0.206)\tAcc@1  87.50 ( 95.19)\tAcc@5 100.00 (100.00)\tLoss 1.3406e-01 (1.3927e-01)\n",
      "[[ 386   44   12   16    8]\n",
      " [  26 1372   15   28    2]\n",
      " [   6    9  718   39    1]\n",
      " [   5   23   26 2602    2]\n",
      " [   1    3    2    4  306]]\n",
      "Epoch 69 perf acc@1: 95.19094848632812, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.19094848632812, perf acc@5: 100.0 at epoch 69\n",
      "Epoch: 70 / 100\n",
      "Epoch: [70][  1/707]\tTime  1.274 ( 1.274)\tData  1.119 ( 1.119)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 7.8688e-02 (7.8688e-02)\n",
      "Epoch: [70][178/707]\tTime  0.342 ( 0.346)\tData  0.208 ( 0.210)\tAcc@1 100.00 ( 95.72)\tAcc@5 100.00 (100.00)\tLoss 1.8266e-02 (1.0836e-01)\n",
      "Epoch: [70][355/707]\tTime  0.357 ( 0.343)\tData  0.207 ( 0.207)\tAcc@1  87.50 ( 95.53)\tAcc@5 100.00 (100.00)\tLoss 5.3472e-01 (1.2536e-01)\n",
      "Epoch: [70][532/707]\tTime  0.351 ( 0.342)\tData  0.208 ( 0.207)\tAcc@1 100.00 ( 95.25)\tAcc@5 100.00 (100.00)\tLoss 2.7377e-02 (1.3255e-01)\n",
      "Epoch: [70][707/707]\tTime  0.337 ( 0.341)\tData  0.213 ( 0.207)\tAcc@1 100.00 ( 94.98)\tAcc@5 100.00 (100.00)\tLoss 9.3165e-02 (1.4134e-01)\n",
      "[[ 400   40   12   14    0]\n",
      " [  32 1354   11   42    4]\n",
      " [   2   15  711   44    1]\n",
      " [   8   25   27 2596    2]\n",
      " [   2    3    0    0  311]]\n",
      "Epoch 70 perf acc@1: 94.9787826538086, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.19094848632812, perf acc@5: 100.0 at epoch 69\n",
      "Decreasing learning rate to 0.00025\n",
      "Epoch: 71 / 100\n",
      "Epoch: [71][  1/707]\tTime  1.209 ( 1.209)\tData  1.055 ( 1.055)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.1864e-01 (1.1864e-01)\n",
      "Epoch: [71][178/707]\tTime  0.310 ( 0.346)\tData  0.177 ( 0.208)\tAcc@1 100.00 ( 95.58)\tAcc@5 100.00 (100.00)\tLoss 1.0322e-01 (1.3043e-01)\n",
      "Epoch: [71][355/707]\tTime  0.348 ( 0.343)\tData  0.206 ( 0.206)\tAcc@1 100.00 ( 95.81)\tAcc@5 100.00 (100.00)\tLoss 6.4564e-02 (1.3168e-01)\n",
      "Epoch: [71][532/707]\tTime  0.340 ( 0.343)\tData  0.204 ( 0.205)\tAcc@1  87.50 ( 95.79)\tAcc@5 100.00 (100.00)\tLoss 3.1854e-01 (1.2817e-01)\n",
      "Epoch: [71][707/707]\tTime  0.335 ( 0.342)\tData  0.210 ( 0.205)\tAcc@1 100.00 ( 95.65)\tAcc@5 100.00 (100.00)\tLoss 8.5923e-02 (1.3407e-01)\n",
      "[[ 402   36   12   13    3]\n",
      " [  26 1377   13   23    4]\n",
      " [   5   10  720   38    0]\n",
      " [   5   24   20 2605    4]\n",
      " [   2    0    1    7  306]]\n",
      "Epoch 71 perf acc@1: 95.650634765625, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.650634765625, perf acc@5: 100.0 at epoch 71\n",
      "Epoch: 72 / 100\n",
      "Epoch: [72][  1/707]\tTime  1.343 ( 1.343)\tData  1.196 ( 1.196)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 6.3329e-03 (6.3329e-03)\n",
      "Epoch: [72][178/707]\tTime  0.324 ( 0.347)\tData  0.194 ( 0.208)\tAcc@1  75.00 ( 94.94)\tAcc@5 100.00 (100.00)\tLoss 6.4653e-01 (1.3915e-01)\n",
      "Epoch: [72][355/707]\tTime  0.340 ( 0.343)\tData  0.204 ( 0.206)\tAcc@1  87.50 ( 95.00)\tAcc@5 100.00 (100.00)\tLoss 2.6112e-01 (1.4676e-01)\n",
      "Epoch: [72][532/707]\tTime  0.325 ( 0.342)\tData  0.195 ( 0.205)\tAcc@1  87.50 ( 95.25)\tAcc@5 100.00 (100.00)\tLoss 2.6264e-01 (1.4334e-01)\n",
      "Epoch: [72][707/707]\tTime  0.339 ( 0.342)\tData  0.216 ( 0.206)\tAcc@1 100.00 ( 95.12)\tAcc@5 100.00 (100.00)\tLoss 3.3505e-02 (1.4498e-01)\n",
      "[[ 393   44   13   16    0]\n",
      " [  30 1368   12   33    0]\n",
      " [   6   11  714   39    3]\n",
      " [   7   19   26 2598    8]\n",
      " [   2    2    1    4  307]]\n",
      "Epoch 72 perf acc@1: 95.12023162841797, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.650634765625, perf acc@5: 100.0 at epoch 71\n",
      "Decreasing learning rate to 0.00023\n",
      "Epoch: 73 / 100\n",
      "Epoch: [73][  1/707]\tTime  1.201 ( 1.201)\tData  1.060 ( 1.060)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.7746e-01 (2.7746e-01)\n",
      "Epoch: [73][178/707]\tTime  0.336 ( 0.346)\tData  0.206 ( 0.208)\tAcc@1 100.00 ( 94.94)\tAcc@5 100.00 (100.00)\tLoss 4.8423e-03 (1.3755e-01)\n",
      "Epoch: [73][355/707]\tTime  0.350 ( 0.343)\tData  0.218 ( 0.207)\tAcc@1  62.50 ( 94.86)\tAcc@5 100.00 (100.00)\tLoss 9.3755e-01 (1.3672e-01)\n",
      "Epoch: [73][532/707]\tTime  0.355 ( 0.342)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 94.97)\tAcc@5 100.00 (100.00)\tLoss 2.1675e-02 (1.3610e-01)\n",
      "Epoch: [73][707/707]\tTime  0.335 ( 0.342)\tData  0.212 ( 0.206)\tAcc@1 100.00 ( 95.21)\tAcc@5 100.00 (100.00)\tLoss 7.8752e-02 (1.3396e-01)\n",
      "[[ 394   43   12   16    1]\n",
      " [  24 1372    9   34    4]\n",
      " [   4   14  716   38    1]\n",
      " [   8   21   24 2597    8]\n",
      " [   2    1    0    7  306]]\n",
      "Epoch 73 perf acc@1: 95.20863342285156, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.650634765625, perf acc@5: 100.0 at epoch 71\n",
      "Epoch: 74 / 100\n",
      "Epoch: [74][  1/707]\tTime  1.265 ( 1.265)\tData  1.107 ( 1.107)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.4999e-01 (2.4999e-01)\n",
      "Epoch: [74][178/707]\tTime  0.324 ( 0.347)\tData  0.196 ( 0.210)\tAcc@1 100.00 ( 95.86)\tAcc@5 100.00 (100.00)\tLoss 3.1844e-02 (1.2978e-01)\n",
      "Epoch: [74][355/707]\tTime  0.329 ( 0.344)\tData  0.200 ( 0.207)\tAcc@1 100.00 ( 95.49)\tAcc@5 100.00 (100.00)\tLoss 8.2121e-02 (1.3099e-01)\n",
      "Epoch: [74][532/707]\tTime  0.336 ( 0.343)\tData  0.195 ( 0.206)\tAcc@1 100.00 ( 95.49)\tAcc@5 100.00 (100.00)\tLoss 1.8843e-01 (1.3388e-01)\n",
      "Epoch: [74][707/707]\tTime  0.332 ( 0.342)\tData  0.209 ( 0.206)\tAcc@1 100.00 ( 95.24)\tAcc@5 100.00 (100.00)\tLoss 2.6645e-02 (1.3552e-01)\n",
      "[[ 400   37   11   12    6]\n",
      " [  31 1361   13   35    3]\n",
      " [   9   12  722   29    1]\n",
      " [   5   26   26 2596    5]\n",
      " [   1    5    1    1  308]]\n",
      "Epoch 74 perf acc@1: 95.24398803710938, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.650634765625, perf acc@5: 100.0 at epoch 71\n",
      "Decreasing learning rate to 0.00020\n",
      "Epoch: 75 / 100\n",
      "Epoch: [75][  1/707]\tTime  1.302 ( 1.302)\tData  1.152 ( 1.152)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 5.7081e-02 (5.7081e-02)\n",
      "Epoch: [75][178/707]\tTime  0.342 ( 0.347)\tData  0.210 ( 0.210)\tAcc@1 100.00 ( 95.44)\tAcc@5 100.00 (100.00)\tLoss 3.0547e-02 (1.4336e-01)\n",
      "Epoch: [75][355/707]\tTime  0.343 ( 0.344)\tData  0.196 ( 0.206)\tAcc@1  87.50 ( 94.82)\tAcc@5 100.00 (100.00)\tLoss 1.4512e-01 (1.6653e-01)\n",
      "Epoch: [75][532/707]\tTime  0.349 ( 0.342)\tData  0.210 ( 0.206)\tAcc@1 100.00 ( 95.04)\tAcc@5 100.00 (100.00)\tLoss 5.1174e-02 (1.5629e-01)\n",
      "Epoch: [75][707/707]\tTime  0.337 ( 0.342)\tData  0.215 ( 0.206)\tAcc@1 100.00 ( 95.28)\tAcc@5 100.00 (100.00)\tLoss 1.7446e-01 (1.4948e-01)\n",
      "[[ 392   42    8   22    2]\n",
      " [  24 1368   15   34    2]\n",
      " [   6   20  712   35    0]\n",
      " [   9   15   23 2608    3]\n",
      " [   3    3    0    1  309]]\n",
      "Epoch 75 perf acc@1: 95.27935028076172, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.650634765625, perf acc@5: 100.0 at epoch 71\n",
      "Epoch: 76 / 100\n",
      "Epoch: [76][  1/707]\tTime  0.987 ( 0.987)\tData  0.829 ( 0.829)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.2512e-01 (2.2512e-01)\n",
      "Epoch: [76][178/707]\tTime  0.336 ( 0.345)\tData  0.203 ( 0.207)\tAcc@1 100.00 ( 94.52)\tAcc@5 100.00 (100.00)\tLoss 3.5924e-03 (1.4442e-01)\n",
      "Epoch: [76][355/707]\tTime  0.341 ( 0.343)\tData  0.209 ( 0.206)\tAcc@1 100.00 ( 95.28)\tAcc@5 100.00 (100.00)\tLoss 2.1311e-03 (1.3889e-01)\n",
      "Epoch: [76][532/707]\tTime  0.354 ( 0.343)\tData  0.207 ( 0.205)\tAcc@1 100.00 ( 95.42)\tAcc@5 100.00 (100.00)\tLoss 1.0182e-01 (1.3438e-01)\n",
      "Epoch: [76][707/707]\tTime  0.336 ( 0.342)\tData  0.212 ( 0.205)\tAcc@1  62.50 ( 95.44)\tAcc@5 100.00 (100.00)\tLoss 1.0862e+00 (1.3608e-01)\n",
      "[[ 397   42   12   14    1]\n",
      " [  18 1374   18   31    2]\n",
      " [   7   17  710   37    2]\n",
      " [   9   23   17 2605    4]\n",
      " [   1    0    1    2  312]]\n",
      "Epoch 76 perf acc@1: 95.4384765625, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.650634765625, perf acc@5: 100.0 at epoch 71\n",
      "Decreasing learning rate to 0.00018\n",
      "Epoch: 77 / 100\n",
      "Epoch: [77][  1/707]\tTime  1.321 ( 1.321)\tData  1.167 ( 1.167)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.7808e-01 (2.7808e-01)\n",
      "Epoch: [77][178/707]\tTime  0.342 ( 0.347)\tData  0.208 ( 0.209)\tAcc@1  87.50 ( 95.79)\tAcc@5 100.00 (100.00)\tLoss 1.3574e-01 (1.2042e-01)\n",
      "Epoch: [77][355/707]\tTime  0.349 ( 0.344)\tData  0.211 ( 0.207)\tAcc@1 100.00 ( 95.49)\tAcc@5 100.00 (100.00)\tLoss 6.9235e-02 (1.2951e-01)\n",
      "Epoch: [77][532/707]\tTime  0.338 ( 0.343)\tData  0.207 ( 0.206)\tAcc@1  87.50 ( 95.21)\tAcc@5 100.00 (100.00)\tLoss 2.3863e-01 (1.3731e-01)\n",
      "Epoch: [77][707/707]\tTime  0.332 ( 0.342)\tData  0.209 ( 0.205)\tAcc@1 100.00 ( 95.23)\tAcc@5 100.00 (100.00)\tLoss 1.2096e-01 (1.3491e-01)\n",
      "[[ 397   39    7   23    0]\n",
      " [  33 1358   17   32    3]\n",
      " [   6    9  720   34    4]\n",
      " [   7   16   25 2606    4]\n",
      " [   2    2    2    5  305]]\n",
      "Epoch 77 perf acc@1: 95.22631072998047, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.650634765625, perf acc@5: 100.0 at epoch 71\n",
      "Epoch: 78 / 100\n",
      "Epoch: [78][  1/707]\tTime  1.304 ( 1.304)\tData  1.138 ( 1.138)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 1.7420e-01 (1.7420e-01)\n",
      "Epoch: [78][178/707]\tTime  0.340 ( 0.347)\tData  0.206 ( 0.209)\tAcc@1 100.00 ( 96.35)\tAcc@5 100.00 (100.00)\tLoss 5.6431e-02 (1.1020e-01)\n",
      "Epoch: [78][355/707]\tTime  0.348 ( 0.344)\tData  0.207 ( 0.207)\tAcc@1 100.00 ( 95.99)\tAcc@5 100.00 (100.00)\tLoss 1.4732e-02 (1.2228e-01)\n",
      "Epoch: [78][532/707]\tTime  0.334 ( 0.343)\tData  0.202 ( 0.206)\tAcc@1 100.00 ( 95.91)\tAcc@5 100.00 (100.00)\tLoss 4.5899e-03 (1.2068e-01)\n",
      "Epoch: [78][707/707]\tTime  0.339 ( 0.342)\tData  0.214 ( 0.206)\tAcc@1  87.50 ( 95.85)\tAcc@5 100.00 (100.00)\tLoss 2.7176e-01 (1.2277e-01)\n",
      "[[ 413   26   10   17    0]\n",
      " [  24 1374    7   32    6]\n",
      " [  10    7  718   37    1]\n",
      " [   4   24   19 2606    5]\n",
      " [   1    1    2    2  310]]\n",
      "Epoch 78 perf acc@1: 95.84512329101562, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.84512329101562, perf acc@5: 100.0 at epoch 78\n",
      "Decreasing learning rate to 0.00016\n",
      "Epoch: 79 / 100\n",
      "Epoch: [79][  1/707]\tTime  1.248 ( 1.248)\tData  1.108 ( 1.108)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 8.3572e-02 (8.3572e-02)\n",
      "Epoch: [79][178/707]\tTime  0.332 ( 0.347)\tData  0.193 ( 0.207)\tAcc@1 100.00 ( 96.56)\tAcc@5 100.00 (100.00)\tLoss 2.7758e-02 (1.1033e-01)\n",
      "Epoch: [79][355/707]\tTime  0.331 ( 0.343)\tData  0.203 ( 0.206)\tAcc@1 100.00 ( 96.06)\tAcc@5 100.00 (100.00)\tLoss 3.8924e-02 (1.1699e-01)\n",
      "Epoch: [79][532/707]\tTime  0.335 ( 0.343)\tData  0.202 ( 0.205)\tAcc@1 100.00 ( 95.94)\tAcc@5 100.00 (100.00)\tLoss 3.1808e-02 (1.1956e-01)\n",
      "Epoch: [79][707/707]\tTime  0.333 ( 0.342)\tData  0.210 ( 0.205)\tAcc@1 100.00 ( 95.72)\tAcc@5 100.00 (100.00)\tLoss 1.5954e-02 (1.2523e-01)\n",
      "[[ 402   34    9   19    2]\n",
      " [  27 1370   11   32    3]\n",
      " [   3   12  726   32    0]\n",
      " [   4   24   19 2610    1]\n",
      " [   2    4    0    4  306]]\n",
      "Epoch 79 perf acc@1: 95.72135925292969, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.84512329101562, perf acc@5: 100.0 at epoch 78\n",
      "Epoch: 80 / 100\n",
      "Epoch: [80][  1/707]\tTime  1.105 ( 1.105)\tData  0.927 ( 0.927)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 9.7648e-02 (9.7648e-02)\n",
      "Epoch: [80][178/707]\tTime  0.347 ( 0.346)\tData  0.200 ( 0.209)\tAcc@1  87.50 ( 95.37)\tAcc@5 100.00 (100.00)\tLoss 2.4591e-01 (1.2499e-01)\n",
      "Epoch: [80][355/707]\tTime  0.342 ( 0.343)\tData  0.208 ( 0.207)\tAcc@1 100.00 ( 95.60)\tAcc@5 100.00 (100.00)\tLoss 2.6731e-02 (1.2078e-01)\n",
      "Epoch: [80][532/707]\tTime  0.332 ( 0.343)\tData  0.200 ( 0.206)\tAcc@1 100.00 ( 95.37)\tAcc@5 100.00 (100.00)\tLoss 1.3424e-01 (1.2487e-01)\n",
      "Epoch: [80][707/707]\tTime  0.331 ( 0.342)\tData  0.208 ( 0.206)\tAcc@1  87.50 ( 95.51)\tAcc@5 100.00 (100.00)\tLoss 2.5764e-01 (1.2399e-01)\n",
      "[[ 402   34   11   17    2]\n",
      " [  27 1375   10   28    3]\n",
      " [   7   11  709   46    0]\n",
      " [   6   26   21 2604    1]\n",
      " [   0    1    2    1  312]]\n",
      "Epoch 80 perf acc@1: 95.50919342041016, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.84512329101562, perf acc@5: 100.0 at epoch 78\n",
      "Decreasing learning rate to 0.00015\n",
      "Epoch: 81 / 100\n",
      "Epoch: [81][  1/707]\tTime  1.284 ( 1.284)\tData  1.105 ( 1.105)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 2.5440e-02 (2.5440e-02)\n",
      "Epoch: [81][178/707]\tTime  0.345 ( 0.346)\tData  0.205 ( 0.209)\tAcc@1  87.50 ( 96.84)\tAcc@5 100.00 (100.00)\tLoss 1.1958e+00 (9.7540e-02)\n",
      "Epoch: [81][355/707]\tTime  0.337 ( 0.344)\tData  0.202 ( 0.207)\tAcc@1 100.00 ( 96.02)\tAcc@5 100.00 (100.00)\tLoss 1.2697e-01 (1.1426e-01)\n",
      "Epoch: [81][532/707]\tTime  0.348 ( 0.343)\tData  0.211 ( 0.207)\tAcc@1  87.50 ( 96.03)\tAcc@5 100.00 (100.00)\tLoss 2.6542e-01 (1.1444e-01)\n",
      "Epoch: [81][707/707]\tTime  0.338 ( 0.342)\tData  0.214 ( 0.206)\tAcc@1 100.00 ( 95.92)\tAcc@5 100.00 (100.00)\tLoss 6.3838e-03 (1.1954e-01)\n",
      "[[ 405   35    9   16    1]\n",
      " [  21 1377   11   32    2]\n",
      " [   7   15  725   26    0]\n",
      " [   6   14   26 2609    3]\n",
      " [   2    1    0    4  309]]\n",
      "Epoch 81 perf acc@1: 95.91584014892578, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.91584014892578, perf acc@5: 100.0 at epoch 81\n",
      "Epoch: 82 / 100\n",
      "Epoch: [82][  1/707]\tTime  1.319 ( 1.319)\tData  1.168 ( 1.168)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 9.8081e-02 (9.8081e-02)\n",
      "Epoch: [82][178/707]\tTime  0.347 ( 0.347)\tData  0.201 ( 0.210)\tAcc@1  87.50 ( 95.37)\tAcc@5 100.00 (100.00)\tLoss 1.1446e-01 (1.2744e-01)\n",
      "Epoch: [82][355/707]\tTime  0.357 ( 0.344)\tData  0.212 ( 0.206)\tAcc@1 100.00 ( 95.14)\tAcc@5 100.00 (100.00)\tLoss 1.6043e-01 (1.3642e-01)\n",
      "Epoch: [82][532/707]\tTime  0.330 ( 0.343)\tData  0.188 ( 0.206)\tAcc@1 100.00 ( 95.18)\tAcc@5 100.00 (100.00)\tLoss 1.3233e-02 (1.3343e-01)\n",
      "Epoch: [82][707/707]\tTime  0.340 ( 0.342)\tData  0.217 ( 0.205)\tAcc@1 100.00 ( 95.60)\tAcc@5 100.00 (100.00)\tLoss 4.7216e-03 (1.2536e-01)\n",
      "[[ 408   34    9   15    0]\n",
      " [  29 1371   12   29    2]\n",
      " [   4   14  718   36    1]\n",
      " [   8   25   22 2599    4]\n",
      " [   1    2    1    1  311]]\n",
      "Epoch 82 perf acc@1: 95.59759521484375, perf acc@5: 100.0\n",
      "Best perf acc@1: 95.91584014892578, perf acc@5: 100.0 at epoch 81\n",
      "Decreasing learning rate to 0.00013\n",
      "Epoch: 83 / 100\n",
      "Epoch: [83][  1/707]\tTime  1.254 ( 1.254)\tData  1.070 ( 1.070)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 3.4877e-02 (3.4877e-02)\n",
      "Epoch: [83][178/707]\tTime  0.328 ( 0.346)\tData  0.200 ( 0.208)\tAcc@1 100.00 ( 96.77)\tAcc@5 100.00 (100.00)\tLoss 7.5856e-03 (1.0664e-01)\n",
      "Epoch: [83][355/707]\tTime  0.341 ( 0.343)\tData  0.207 ( 0.207)\tAcc@1 100.00 ( 96.41)\tAcc@5 100.00 (100.00)\tLoss 9.5750e-02 (1.1567e-01)\n",
      "Epoch: [83][532/707]\tTime  0.333 ( 0.343)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 96.15)\tAcc@5 100.00 (100.00)\tLoss 6.0215e-03 (1.1584e-01)\n",
      "Epoch: [83][707/707]\tTime  0.338 ( 0.342)\tData  0.214 ( 0.206)\tAcc@1 100.00 ( 96.34)\tAcc@5 100.00 (100.00)\tLoss 2.1301e-02 (1.1357e-01)\n",
      "[[ 424   26    3   11    2]\n",
      " [  17 1375    9   40    2]\n",
      " [   6    8  728   30    1]\n",
      " [   4   19   21 2612    2]\n",
      " [   0    1    2    3  310]]\n",
      "Epoch 83 perf acc@1: 96.34017181396484, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 84 / 100\n",
      "Epoch: [84][  1/707]\tTime  1.224 ( 1.224)\tData  1.073 ( 1.073)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 3.5493e-01 (3.5493e-01)\n",
      "Epoch: [84][178/707]\tTime  0.333 ( 0.347)\tData  0.197 ( 0.210)\tAcc@1 100.00 ( 96.63)\tAcc@5 100.00 (100.00)\tLoss 1.0129e-02 (1.0200e-01)\n",
      "Epoch: [84][355/707]\tTime  0.334 ( 0.344)\tData  0.204 ( 0.207)\tAcc@1 100.00 ( 96.20)\tAcc@5 100.00 (100.00)\tLoss 6.6566e-03 (1.1681e-01)\n",
      "Epoch: [84][532/707]\tTime  0.356 ( 0.343)\tData  0.213 ( 0.206)\tAcc@1 100.00 ( 95.96)\tAcc@5 100.00 (100.00)\tLoss 2.5173e-02 (1.2220e-01)\n",
      "Epoch: [84][707/707]\tTime  0.338 ( 0.342)\tData  0.214 ( 0.206)\tAcc@1 100.00 ( 95.83)\tAcc@5 100.00 (100.00)\tLoss 1.2955e-01 (1.2255e-01)\n",
      "[[ 408   39    6   13    0]\n",
      " [  25 1370   13   33    2]\n",
      " [   4    7  725   36    1]\n",
      " [   8   17   24 2607    2]\n",
      " [   4    1    0    1  310]]\n",
      "Epoch 84 perf acc@1: 95.82743835449219, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Decreasing learning rate to 0.00012\n",
      "Epoch: 85 / 100\n",
      "Epoch: [85][  1/707]\tTime  1.217 ( 1.217)\tData  1.065 ( 1.065)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 9.2501e-02 (9.2501e-02)\n",
      "Epoch: [85][178/707]\tTime  0.344 ( 0.346)\tData  0.205 ( 0.209)\tAcc@1 100.00 ( 95.51)\tAcc@5 100.00 (100.00)\tLoss 2.5198e-02 (1.3014e-01)\n",
      "Epoch: [85][355/707]\tTime  0.327 ( 0.343)\tData  0.194 ( 0.207)\tAcc@1  87.50 ( 95.88)\tAcc@5 100.00 (100.00)\tLoss 2.7608e-01 (1.1831e-01)\n",
      "Epoch: [85][532/707]\tTime  0.352 ( 0.342)\tData  0.210 ( 0.206)\tAcc@1 100.00 ( 95.82)\tAcc@5 100.00 (100.00)\tLoss 1.0692e-01 (1.1946e-01)\n",
      "Epoch: [85][707/707]\tTime  0.340 ( 0.342)\tData  0.213 ( 0.206)\tAcc@1 100.00 ( 95.79)\tAcc@5 100.00 (100.00)\tLoss 5.3166e-02 (1.2006e-01)\n",
      "[[ 405   34    9   17    1]\n",
      " [  20 1373    9   39    2]\n",
      " [   4    8  731   27    3]\n",
      " [   4   22   23 2606    3]\n",
      " [   5    1    3    4  303]]\n",
      "Epoch 85 perf acc@1: 95.79208374023438, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 86 / 100\n",
      "Epoch: [86][  1/707]\tTime  1.304 ( 1.304)\tData  1.152 ( 1.152)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.1573e-01 (1.1573e-01)\n",
      "Epoch: [86][178/707]\tTime  0.330 ( 0.348)\tData  0.200 ( 0.209)\tAcc@1  87.50 ( 96.07)\tAcc@5 100.00 (100.00)\tLoss 5.8039e-01 (1.2214e-01)\n",
      "Epoch: [86][355/707]\tTime  0.329 ( 0.345)\tData  0.196 ( 0.206)\tAcc@1 100.00 ( 95.85)\tAcc@5 100.00 (100.00)\tLoss 5.9425e-03 (1.3276e-01)\n",
      "Epoch: [86][532/707]\tTime  0.342 ( 0.343)\tData  0.209 ( 0.205)\tAcc@1 100.00 ( 96.08)\tAcc@5 100.00 (100.00)\tLoss 3.9002e-02 (1.2528e-01)\n",
      "Epoch: [86][707/707]\tTime  0.339 ( 0.343)\tData  0.213 ( 0.205)\tAcc@1  87.50 ( 96.13)\tAcc@5 100.00 (100.00)\tLoss 2.6758e-01 (1.2366e-01)\n",
      "[[ 415   25    7   18    1]\n",
      " [  20 1387    9   24    3]\n",
      " [   9   13  715   34    2]\n",
      " [   2   19   24 2610    3]\n",
      " [   2    1    1    2  310]]\n",
      "Epoch 86 perf acc@1: 96.12800598144531, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Decreasing learning rate to 0.00011\n",
      "Epoch: 87 / 100\n",
      "Epoch: [87][  1/707]\tTime  1.309 ( 1.309)\tData  1.145 ( 1.145)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLoss 2.2365e-01 (2.2365e-01)\n",
      "Epoch: [87][178/707]\tTime  0.339 ( 0.346)\tData  0.204 ( 0.210)\tAcc@1 100.00 ( 96.63)\tAcc@5 100.00 (100.00)\tLoss 1.0496e-02 (1.0752e-01)\n",
      "Epoch: [87][355/707]\tTime  0.344 ( 0.343)\tData  0.206 ( 0.207)\tAcc@1 100.00 ( 96.44)\tAcc@5 100.00 (100.00)\tLoss 1.3589e-01 (1.0905e-01)\n",
      "Epoch: [87][532/707]\tTime  0.356 ( 0.342)\tData  0.199 ( 0.207)\tAcc@1  87.50 ( 96.48)\tAcc@5 100.00 (100.00)\tLoss 2.6955e-01 (1.1178e-01)\n",
      "Epoch: [87][707/707]\tTime  0.335 ( 0.342)\tData  0.211 ( 0.206)\tAcc@1 100.00 ( 96.25)\tAcc@5 100.00 (100.00)\tLoss 6.3130e-02 (1.1949e-01)\n",
      "[[ 410   29    9   17    1]\n",
      " [  20 1385    9   29    0]\n",
      " [   5   12  728   27    1]\n",
      " [   4   22   19 2607    6]\n",
      " [   1    1    0    0  314]]\n",
      "Epoch 87 perf acc@1: 96.25177001953125, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 88 / 100\n",
      "Epoch: [88][  1/707]\tTime  1.289 ( 1.289)\tData  1.143 ( 1.143)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 1.6153e-02 (1.6153e-02)\n",
      "Epoch: [88][178/707]\tTime  0.343 ( 0.346)\tData  0.192 ( 0.208)\tAcc@1 100.00 ( 96.56)\tAcc@5 100.00 (100.00)\tLoss 2.2468e-02 (1.0392e-01)\n",
      "Epoch: [88][355/707]\tTime  0.350 ( 0.344)\tData  0.206 ( 0.206)\tAcc@1 100.00 ( 96.13)\tAcc@5 100.00 (100.00)\tLoss 4.2003e-02 (1.1018e-01)\n",
      "Epoch: [88][532/707]\tTime  0.335 ( 0.343)\tData  0.205 ( 0.206)\tAcc@1 100.00 ( 96.22)\tAcc@5 100.00 (100.00)\tLoss 2.6937e-02 (1.1033e-01)\n",
      "Epoch: [88][707/707]\tTime  0.340 ( 0.342)\tData  0.213 ( 0.205)\tAcc@1 100.00 ( 96.29)\tAcc@5 100.00 (100.00)\tLoss 6.7652e-03 (1.1058e-01)\n",
      "[[ 417   26   10   13    0]\n",
      " [  21 1376   15   28    3]\n",
      " [   4   12  726   31    0]\n",
      " [   6   21   13 2616    2]\n",
      " [   1    1    1    2  311]]\n",
      "Epoch 88 perf acc@1: 96.2871322631836, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Decreasing learning rate to 0.00010\n",
      "Epoch: 89 / 100\n",
      "Epoch: [89][  1/707]\tTime  1.186 ( 1.186)\tData  1.036 ( 1.036)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 5.2903e-03 (5.2903e-03)\n",
      "Epoch: [89][178/707]\tTime  0.349 ( 0.347)\tData  0.207 ( 0.209)\tAcc@1 100.00 ( 96.28)\tAcc@5 100.00 (100.00)\tLoss 1.9837e-02 (1.0289e-01)\n",
      "Epoch: [89][355/707]\tTime  0.338 ( 0.344)\tData  0.203 ( 0.206)\tAcc@1 100.00 ( 96.09)\tAcc@5 100.00 (100.00)\tLoss 7.0848e-03 (1.1118e-01)\n",
      "Epoch: [89][532/707]\tTime  0.336 ( 0.343)\tData  0.202 ( 0.206)\tAcc@1  87.50 ( 95.82)\tAcc@5 100.00 (100.00)\tLoss 4.8712e-01 (1.1638e-01)\n",
      "Epoch: [89][707/707]\tTime  0.342 ( 0.342)\tData  0.216 ( 0.205)\tAcc@1 100.00 ( 95.90)\tAcc@5 100.00 (100.00)\tLoss 9.1787e-03 (1.1145e-01)\n",
      "[[ 409   39    8   10    0]\n",
      " [  28 1377   11   27    0]\n",
      " [   8   12  722   28    3]\n",
      " [   6   13   26 2608    5]\n",
      " [   1    2    2    3  308]]\n",
      "Epoch 89 perf acc@1: 95.89816284179688, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 90 / 100\n",
      "Epoch: [90][  1/707]\tTime  1.309 ( 1.309)\tData  1.159 ( 1.159)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 2.9159e-02 (2.9159e-02)\n",
      "Epoch: [90][178/707]\tTime  0.353 ( 0.346)\tData  0.203 ( 0.208)\tAcc@1 100.00 ( 95.51)\tAcc@5 100.00 (100.00)\tLoss 2.3226e-02 (1.1472e-01)\n",
      "Epoch: [90][355/707]\tTime  0.347 ( 0.344)\tData  0.202 ( 0.206)\tAcc@1 100.00 ( 95.81)\tAcc@5 100.00 (100.00)\tLoss 8.2853e-03 (1.1518e-01)\n",
      "Epoch: [90][532/707]\tTime  0.342 ( 0.342)\tData  0.205 ( 0.205)\tAcc@1 100.00 ( 96.24)\tAcc@5 100.00 (100.00)\tLoss 1.0539e-01 (1.0751e-01)\n",
      "Epoch: [90][707/707]\tTime  0.339 ( 0.342)\tData  0.213 ( 0.205)\tAcc@1 100.00 ( 95.97)\tAcc@5 100.00 (100.00)\tLoss 2.6162e-03 (1.1275e-01)\n",
      "[[ 410   37    6   13    0]\n",
      " [  26 1375   12   28    2]\n",
      " [   3    9  732   26    3]\n",
      " [   5   25   22 2602    4]\n",
      " [   1    2    0    4  309]]\n",
      "Epoch 90 perf acc@1: 95.96888732910156, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Decreasing learning rate to 0.00009\n",
      "Epoch: 91 / 100\n",
      "Epoch: [91][  1/707]\tTime  1.169 ( 1.169)\tData  1.005 ( 1.005)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 9.5511e-03 (9.5511e-03)\n",
      "Epoch: [91][178/707]\tTime  0.348 ( 0.347)\tData  0.202 ( 0.207)\tAcc@1 100.00 ( 95.51)\tAcc@5 100.00 (100.00)\tLoss 1.8189e-02 (1.2731e-01)\n",
      "Epoch: [91][355/707]\tTime  0.322 ( 0.344)\tData  0.192 ( 0.205)\tAcc@1 100.00 ( 95.81)\tAcc@5 100.00 (100.00)\tLoss 1.1441e-01 (1.2465e-01)\n",
      "Epoch: [91][532/707]\tTime  0.358 ( 0.343)\tData  0.213 ( 0.205)\tAcc@1  75.00 ( 96.05)\tAcc@5 100.00 (100.00)\tLoss 2.1841e-01 (1.1961e-01)\n",
      "Epoch: [91][707/707]\tTime  0.334 ( 0.342)\tData  0.211 ( 0.205)\tAcc@1  87.50 ( 96.13)\tAcc@5 100.00 (100.00)\tLoss 1.8205e-01 (1.1737e-01)\n",
      "[[ 414   28    8   12    4]\n",
      " [  21 1378    8   35    1]\n",
      " [   4   18  717   33    1]\n",
      " [   0   17   24 2617    0]\n",
      " [   1    1    0    3  311]]\n",
      "Epoch 91 perf acc@1: 96.12800598144531, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 92 / 100\n",
      "Epoch: [92][  1/707]\tTime  1.314 ( 1.314)\tData  1.158 ( 1.158)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.7622e-03 (4.7622e-03)\n",
      "Epoch: [92][178/707]\tTime  0.337 ( 0.347)\tData  0.210 ( 0.209)\tAcc@1 100.00 ( 96.00)\tAcc@5 100.00 (100.00)\tLoss 8.0468e-02 (1.2138e-01)\n",
      "Epoch: [92][355/707]\tTime  0.339 ( 0.344)\tData  0.208 ( 0.207)\tAcc@1 100.00 ( 95.32)\tAcc@5 100.00 (100.00)\tLoss 1.3610e-02 (1.2883e-01)\n",
      "Epoch: [92][532/707]\tTime  0.330 ( 0.343)\tData  0.201 ( 0.206)\tAcc@1 100.00 ( 95.54)\tAcc@5 100.00 (100.00)\tLoss 3.2071e-02 (1.2231e-01)\n",
      "Epoch: [92][707/707]\tTime  0.342 ( 0.342)\tData  0.215 ( 0.205)\tAcc@1 100.00 ( 95.56)\tAcc@5 100.00 (100.00)\tLoss 1.0683e-01 (1.2314e-01)\n",
      "[[ 404   33   13   14    2]\n",
      " [  24 1372   13   31    3]\n",
      " [   4   12  725   32    0]\n",
      " [   9   27   24 2596    2]\n",
      " [   0    3    0    5  308]]\n",
      "Epoch 92 perf acc@1: 95.5622329711914, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Decreasing learning rate to 0.00008\n",
      "Epoch: 93 / 100\n",
      "Epoch: [93][  1/707]\tTime  1.353 ( 1.353)\tData  1.204 ( 1.204)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 6.2598e-02 (6.2598e-02)\n",
      "Epoch: [93][178/707]\tTime  0.342 ( 0.347)\tData  0.205 ( 0.210)\tAcc@1 100.00 ( 95.08)\tAcc@5 100.00 (100.00)\tLoss 5.7951e-03 (1.3078e-01)\n",
      "Epoch: [93][355/707]\tTime  0.337 ( 0.344)\tData  0.203 ( 0.207)\tAcc@1 100.00 ( 95.42)\tAcc@5 100.00 (100.00)\tLoss 6.8299e-03 (1.2513e-01)\n",
      "Epoch: [93][532/707]\tTime  0.351 ( 0.343)\tData  0.207 ( 0.207)\tAcc@1 100.00 ( 95.49)\tAcc@5 100.00 (100.00)\tLoss 6.3444e-02 (1.2463e-01)\n",
      "Epoch: [93][707/707]\tTime  0.337 ( 0.342)\tData  0.212 ( 0.206)\tAcc@1 100.00 ( 95.35)\tAcc@5 100.00 (100.00)\tLoss 8.3869e-02 (1.2494e-01)\n",
      "[[ 395   43    7   19    2]\n",
      " [  27 1372    9   33    2]\n",
      " [   8   12  717   36    0]\n",
      " [   5   27   22 2601    3]\n",
      " [   1    2    2    3  308]]\n",
      "Epoch 93 perf acc@1: 95.3500747680664, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 94 / 100\n",
      "Epoch: [94][  1/707]\tTime  1.234 ( 1.234)\tData  1.091 ( 1.091)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 7.1260e-03 (7.1260e-03)\n",
      "Epoch: [94][178/707]\tTime  0.336 ( 0.346)\tData  0.208 ( 0.209)\tAcc@1 100.00 ( 96.35)\tAcc@5 100.00 (100.00)\tLoss 3.4199e-02 (1.0916e-01)\n",
      "Epoch: [94][355/707]\tTime  0.345 ( 0.344)\tData  0.206 ( 0.207)\tAcc@1  87.50 ( 96.30)\tAcc@5 100.00 (100.00)\tLoss 9.7759e-02 (1.0507e-01)\n",
      "Epoch: [94][532/707]\tTime  0.358 ( 0.343)\tData  0.208 ( 0.206)\tAcc@1 100.00 ( 96.17)\tAcc@5 100.00 (100.00)\tLoss 5.3154e-02 (1.0950e-01)\n",
      "Epoch: [94][707/707]\tTime  0.339 ( 0.342)\tData  0.215 ( 0.205)\tAcc@1 100.00 ( 96.27)\tAcc@5 100.00 (100.00)\tLoss 1.4716e-03 (1.0906e-01)\n",
      "[[ 409   30   13   11    3]\n",
      " [  27 1381    9   25    1]\n",
      " [   8   11  727   23    4]\n",
      " [   8   15   15 2619    1]\n",
      " [   0    5    0    2  309]]\n",
      "Epoch 94 perf acc@1: 96.26944732666016, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Decreasing learning rate to 0.00007\n",
      "Epoch: 95 / 100\n",
      "Epoch: [95][  1/707]\tTime  1.320 ( 1.320)\tData  1.170 ( 1.170)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 5.6634e-02 (5.6634e-02)\n",
      "Epoch: [95][178/707]\tTime  0.345 ( 0.346)\tData  0.203 ( 0.207)\tAcc@1  87.50 ( 96.35)\tAcc@5 100.00 (100.00)\tLoss 1.5974e-01 (1.0136e-01)\n",
      "Epoch: [95][355/707]\tTime  0.361 ( 0.344)\tData  0.205 ( 0.205)\tAcc@1 100.00 ( 95.74)\tAcc@5 100.00 (100.00)\tLoss 2.7241e-03 (1.2138e-01)\n",
      "Epoch: [95][532/707]\tTime  0.334 ( 0.343)\tData  0.206 ( 0.205)\tAcc@1 100.00 ( 95.86)\tAcc@5 100.00 (100.00)\tLoss 1.8227e-02 (1.2151e-01)\n",
      "Epoch: [95][707/707]\tTime  0.330 ( 0.342)\tData  0.206 ( 0.205)\tAcc@1 100.00 ( 95.85)\tAcc@5 100.00 (100.00)\tLoss 4.4917e-02 (1.1916e-01)\n",
      "[[ 395   38    9   24    0]\n",
      " [  23 1391    9   20    0]\n",
      " [   9   12  719   31    2]\n",
      " [   8   24   20 2604    2]\n",
      " [   2    1    0    1  312]]\n",
      "Epoch 95 perf acc@1: 95.84512329101562, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 96 / 100\n",
      "Epoch: [96][  1/707]\tTime  1.209 ( 1.209)\tData  1.015 ( 1.015)\tAcc@1  75.00 ( 75.00)\tAcc@5 100.00 (100.00)\tLoss 1.4489e+00 (1.4489e+00)\n",
      "Epoch: [96][178/707]\tTime  0.344 ( 0.346)\tData  0.204 ( 0.208)\tAcc@1 100.00 ( 96.98)\tAcc@5 100.00 (100.00)\tLoss 1.7237e-02 (1.0256e-01)\n",
      "Epoch: [96][355/707]\tTime  0.343 ( 0.343)\tData  0.214 ( 0.206)\tAcc@1 100.00 ( 96.62)\tAcc@5 100.00 (100.00)\tLoss 6.7021e-02 (1.0921e-01)\n",
      "Epoch: [96][532/707]\tTime  0.350 ( 0.342)\tData  0.208 ( 0.206)\tAcc@1 100.00 ( 96.36)\tAcc@5 100.00 (100.00)\tLoss 6.7454e-02 (1.0968e-01)\n",
      "Epoch: [96][707/707]\tTime  0.338 ( 0.342)\tData  0.214 ( 0.205)\tAcc@1 100.00 ( 96.07)\tAcc@5 100.00 (100.00)\tLoss 1.1449e-01 (1.1783e-01)\n",
      "[[ 406   35   12   13    0]\n",
      " [  25 1383    6   29    0]\n",
      " [   8    6  722   36    1]\n",
      " [   6   12   24 2612    4]\n",
      " [   1    0    1    3  311]]\n",
      "Epoch 96 perf acc@1: 96.07496643066406, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Decreasing learning rate to 0.00006\n",
      "Epoch: 97 / 100\n",
      "Epoch: [97][  1/707]\tTime  1.323 ( 1.323)\tData  1.173 ( 1.173)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 5.2171e-03 (5.2171e-03)\n",
      "Epoch: [97][178/707]\tTime  0.349 ( 0.347)\tData  0.210 ( 0.210)\tAcc@1 100.00 ( 97.40)\tAcc@5 100.00 (100.00)\tLoss 1.9283e-02 (8.8053e-02)\n",
      "Epoch: [97][355/707]\tTime  0.335 ( 0.344)\tData  0.205 ( 0.207)\tAcc@1 100.00 ( 96.16)\tAcc@5 100.00 (100.00)\tLoss 1.2611e-02 (1.0850e-01)\n",
      "Epoch: [97][532/707]\tTime  0.323 ( 0.343)\tData  0.192 ( 0.206)\tAcc@1 100.00 ( 96.36)\tAcc@5 100.00 (100.00)\tLoss 6.6656e-02 (1.1166e-01)\n",
      "Epoch: [97][707/707]\tTime  0.337 ( 0.342)\tData  0.214 ( 0.206)\tAcc@1  75.00 ( 96.00)\tAcc@5 100.00 (100.00)\tLoss 2.6431e-01 (1.1914e-01)\n",
      "[[ 399   35   11   20    1]\n",
      " [  25 1380   10   27    1]\n",
      " [   8   11  727   26    1]\n",
      " [   2   26   18 2609    3]\n",
      " [   0    1    0    0  315]]\n",
      "Epoch 97 perf acc@1: 96.00424194335938, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 98 / 100\n",
      "Epoch: [98][  1/707]\tTime  1.135 ( 1.135)\tData  0.987 ( 0.987)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 4.2268e-02 (4.2268e-02)\n",
      "Epoch: [98][178/707]\tTime  0.336 ( 0.346)\tData  0.207 ( 0.208)\tAcc@1  87.50 ( 96.21)\tAcc@5 100.00 (100.00)\tLoss 2.8083e-01 (1.0724e-01)\n",
      "Epoch: [98][355/707]\tTime  0.366 ( 0.343)\tData  0.211 ( 0.206)\tAcc@1  87.50 ( 95.92)\tAcc@5 100.00 (100.00)\tLoss 2.2590e-01 (1.0931e-01)\n",
      "Epoch: [98][532/707]\tTime  0.339 ( 0.342)\tData  0.205 ( 0.205)\tAcc@1 100.00 ( 96.03)\tAcc@5 100.00 (100.00)\tLoss 8.5235e-02 (1.1280e-01)\n",
      "Epoch: [98][707/707]\tTime  0.332 ( 0.342)\tData  0.207 ( 0.205)\tAcc@1 100.00 ( 96.20)\tAcc@5 100.00 (100.00)\tLoss 1.3874e-01 (1.0923e-01)\n",
      "[[ 413   31    8   10    4]\n",
      " [  25 1377   11   30    0]\n",
      " [   4   11  724   31    3]\n",
      " [   7   15   17 2616    3]\n",
      " [   0    3    0    2  311]]\n",
      "Epoch 98 perf acc@1: 96.19873046875, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Decreasing learning rate to 0.00006\n",
      "Epoch: 99 / 100\n",
      "Epoch: [99][  1/707]\tTime  1.132 ( 1.132)\tData  0.974 ( 0.974)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 8.1973e-02 (8.1973e-02)\n",
      "Epoch: [99][178/707]\tTime  0.336 ( 0.346)\tData  0.210 ( 0.208)\tAcc@1 100.00 ( 95.93)\tAcc@5 100.00 (100.00)\tLoss 8.0077e-02 (1.1469e-01)\n",
      "Epoch: [99][355/707]\tTime  0.359 ( 0.343)\tData  0.204 ( 0.206)\tAcc@1 100.00 ( 95.85)\tAcc@5 100.00 (100.00)\tLoss 1.9213e-01 (1.1550e-01)\n",
      "Epoch: [99][532/707]\tTime  0.352 ( 0.342)\tData  0.205 ( 0.206)\tAcc@1  87.50 ( 96.24)\tAcc@5 100.00 (100.00)\tLoss 3.6592e-01 (1.0655e-01)\n",
      "Epoch: [99][707/707]\tTime  0.336 ( 0.342)\tData  0.213 ( 0.206)\tAcc@1 100.00 ( 96.25)\tAcc@5 100.00 (100.00)\tLoss 2.4196e-02 (1.0615e-01)\n",
      "[[ 413   27    9   13    4]\n",
      " [  17 1389   12   21    4]\n",
      " [   8   11  717   36    1]\n",
      " [   6   17   20 2612    3]\n",
      " [   1    1    0    1  313]]\n",
      "Epoch 99 perf acc@1: 96.25177001953125, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n",
      "Epoch: 100 / 100\n",
      "Epoch: [100][  1/707]\tTime  1.224 ( 1.224)\tData  1.064 ( 1.064)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tLoss 5.9533e-03 (5.9533e-03)\n",
      "Epoch: [100][178/707]\tTime  0.353 ( 0.346)\tData  0.209 ( 0.209)\tAcc@1 100.00 ( 96.56)\tAcc@5 100.00 (100.00)\tLoss 1.4429e-02 (1.1123e-01)\n",
      "Epoch: [100][355/707]\tTime  0.341 ( 0.344)\tData  0.209 ( 0.206)\tAcc@1 100.00 ( 96.20)\tAcc@5 100.00 (100.00)\tLoss 6.2055e-02 (1.1034e-01)\n",
      "Epoch: [100][532/707]\tTime  0.332 ( 0.343)\tData  0.202 ( 0.206)\tAcc@1 100.00 ( 96.03)\tAcc@5 100.00 (100.00)\tLoss 4.0846e-03 (1.1511e-01)\n",
      "Epoch: [100][707/707]\tTime  0.337 ( 0.342)\tData  0.211 ( 0.205)\tAcc@1  87.50 ( 95.99)\tAcc@5 100.00 (100.00)\tLoss 3.6035e-01 (1.1509e-01)\n",
      "[[ 398   35   14   17    2]\n",
      " [  19 1382   11   29    2]\n",
      " [  10   14  720   28    1]\n",
      " [   4   14   17 2620    3]\n",
      " [   3    1    0    3  309]]\n",
      "Epoch 100 perf acc@1: 95.98656463623047, perf acc@5: 100.0\n",
      "Best perf acc@1: 96.34017181396484, perf acc@5: 100.0 at epoch 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test data:  1\n",
      "Processed test data:  421\n",
      "Processed test data:  841\n",
      "Processed test data:  1261\n",
      "Processed test data:  1681\n",
      "Processed test data:  2101\n",
      "Processed test data:  2521\n",
      "Processed test data:  2941\n",
      "Processed test data:  3361\n",
      "Processed test data:  3774\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "use_cuda = True\n",
    "model_input_size = 560\n",
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "arch = 'resnet34'\n",
    "optim = 'sgd'\n",
    "lr = 1e-2\n",
    "train_percentage = 1\n",
    "train = True\n",
    "use_extraimages = False\n",
    "validate = False\n",
    "test = True\n",
    "use_weighted_loss = False\n",
    "tencrop_test = False\n",
    "resume_path = ''\n",
    "subset_finetune = False\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    # print(\"Model input size: \", model_input_size)\n",
    "    # print(\"Batch size: \", batch_size)\n",
    "    # print('Arch: ', arch)\n",
    "    # print('Optimizer: ', optim)\n",
    "    # print('Weighted loss: ', use_weighted_loss)\n",
    "\n",
    "    dir_path = os.path.dirname('__file__')\n",
    "\n",
    "    print('Loading dataset and dataloader..')\n",
    "    train_percent = train_percentage\n",
    "    if validate and train_percent == 1:\n",
    "        train_percent = 0.9\n",
    "        print('Warning: train percentage was given 1 with validation enabled, train percentage dropped to 0.9')\n",
    "    train_set, train_loader = get_dataloader(data_split='train', train_percentage=train_percent)\n",
    "    if validate:\n",
    "        val_set, val_loader = get_dataloader(data_split='val', train_percentage=train_percent)\n",
    "    if test:\n",
    "        test_set, test_loader = get_dataloader(data_split='test')\n",
    "    num_classes = len(train_set.classes)\n",
    "\n",
    "    class_weights = [1.0] * num_classes\n",
    "    if use_weighted_loss:\n",
    "        class_weights = train_set.class_weights\n",
    "        print(\"Class weights: \", class_weights)\n",
    "    class_weights = torch.Tensor(class_weights)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(class_weights)\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    best_perf1 = 0\n",
    "    best_perf5 = 0\n",
    "    begin_epoch = 0\n",
    "    best_epoch = 0\n",
    "    state_dict = None  # won't be None if resuming from a trained model\n",
    "    optimizer_dict = None  # won't be None if resuming from a trained model\n",
    "    scheduler_steps = 2  # [30, 60, 90]\n",
    "    scheduler_decay = 0.9  # 0.1\n",
    "\n",
    "    if resume_path:\n",
    "        print('Loading finetuned model from {}..',resume_path)\n",
    "        checkpoint = torch.load(resume_path)\n",
    "        begin_epoch = checkpoint['epoch']\n",
    "        best_epoch = begin_epoch\n",
    "        # best_epoch = checkpoint['best_epoch']\n",
    "        best_perf1 = checkpoint['perf1']\n",
    "        # best_perf5 = checkpoint['perf5']\n",
    "        arch = checkpoint['arch']\n",
    "        num_classes = checkpoint['num_classes']\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        optimizer_dict = checkpoint['optimizer']\n",
    "        print('Begin epoch: ', begin_epoch)\n",
    "        print('Best Acc@1 at epoch {}: {}'.format(best_epoch, best_perf1))\n",
    "        # scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    # arch = 'inceptionv4'\n",
    "    arch = 'resnet34'\n",
    "    model = generate_model(arch, num_classes, state_dict, use_cuda=True)\n",
    "    optimizer = get_optimizer( model, optimizer_dict)\n",
    "    print('Learning rate: {:1.5f}', optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    if train:\n",
    "        for epoch in range(begin_epoch, num_epochs):\n",
    "            print('Epoch: {} / {}'.format(epoch+1, num_epochs))\n",
    "\n",
    "            perf_indicator1, perf_indicator5 = train_epoch(\n",
    "                epoch, train_loader, model, criterion, optimizer, use_cuda=True)\n",
    "\n",
    "            if validate:\n",
    "                perf_indicator1, perf_indicator5 = validate_epoch(\n",
    "                    val_loader, model, criterion, use_cuda=True)\n",
    "\n",
    "            if perf_indicator1 >= best_perf1:\n",
    "                best_perf1 = perf_indicator1\n",
    "                best_perf5 = perf_indicator5\n",
    "                best_epoch = epoch\n",
    "\n",
    "                checkpoint_file = '{}_{}_{}_{}{}{}'.format(\n",
    "                    model_input_size, arch, optim,\n",
    "                    batch_size, '_subset' if subset_finetune else '',\n",
    "                    '_weightedloss' if use_weighted_loss else '')\n",
    "\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'best_epoch': best_epoch + 1,\n",
    "                    'perf1': best_perf1,\n",
    "                    'perf5': best_perf5,\n",
    "                    'arch': arch,\n",
    "                    'num_classes': model.num_classes,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    # 'scheduler': scheduler.state_dict(),\n",
    "                }, dir_path, is_best=True, filename=checkpoint_file)\n",
    "            arch = 'resnet34'\n",
    "            if (epoch+1) % 5 == 0:  # save model every 5 epochs\n",
    "                checkpoint_file = 'Epoch{}_{}_{}_{}_{}{}{}'.format(\n",
    "                    epoch + 1, model_input_size, arch, optim,\n",
    "                    batch_size, '_subset' if subset_finetune else '',\n",
    "                    '_weightedloss' if use_weighted_loss else '')\n",
    "\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'best_epoch': best_epoch + 1,\n",
    "                    'perf1': best_perf1,\n",
    "                    'perf5': best_perf5,\n",
    "                    'arch': arch,\n",
    "                    'num_classes': model.num_classes,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    # 'scheduler': scheduler.state_dict(),\n",
    "                }, dir_path, filename=checkpoint_file)\n",
    "\n",
    "            print('Epoch {} perf acc@1: {}, perf acc@5: {}'.format(\n",
    "                epoch+1, perf_indicator1, perf_indicator5))\n",
    "            print('Best perf acc@1: {}, perf acc@5: {} at epoch {}'.format(\n",
    "                best_perf1, best_perf5, best_epoch+1))\n",
    "            # scheduler.step(perf_indicator1)\n",
    "            if epoch+1 < 100:\n",
    "                adjust_learning_rate(optimizer, epoch+1,  steps=scheduler_steps, dec_rate=scheduler_decay)\n",
    "\n",
    "    if test:\n",
    "        test_cassava(test_loader, model, train_set.classes) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jtWvSt4jubX"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cassava_disease_classification (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04629bbb6ed94433bc54dbcdb761a413": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78c2ee8dacd0426881974c758377b826",
      "max": 87306240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3f88044b56944c092c66dd0a91f6f94",
      "value": 87306240
     }
    },
    "067a6b4d51e244d4b1db17212bbe22f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b449d26691a427d9090732671c32f4f",
      "placeholder": "​",
      "style": "IPY_MODEL_e8d312a677ce4d2ab5eea62dff58285e",
      "value": " 83.3M/83.3M [10:34&lt;00:00, 138kB/s]"
     }
    },
    "6b449d26691a427d9090732671c32f4f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78c2ee8dacd0426881974c758377b826": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b3607084006468fa31902b0c287adaa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3f88044b56944c092c66dd0a91f6f94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d0f94e8c24fc4bd98500ee0db8a75206": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04629bbb6ed94433bc54dbcdb761a413",
       "IPY_MODEL_067a6b4d51e244d4b1db17212bbe22f0"
      ],
      "layout": "IPY_MODEL_9b3607084006468fa31902b0c287adaa"
     }
    },
    "e8d312a677ce4d2ab5eea62dff58285e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
